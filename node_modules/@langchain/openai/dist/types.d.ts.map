{"version":3,"file":"types.d.ts","names":["OpenAI","OpenAIClient","ResponseFormatText","ResponseFormatJSONObject","ResponseFormatJSONSchema","ResponseFormatTextPython","ResponseFormatTextGrammar","TiktokenModel","BaseLanguageModelCallOptions","InteropZodObject","OpenAIChatModelId","ChatModel","NonNullable","OpenAIVerbosityParam","OpenAIBaseInput","Record","OpenAICoreRequestOptions","RequestOptions","OpenAICallOptions","OpenAIInput","OpenAIChatInput","Chat","ChatCompletionMessageParam","ChatCompletionModality","Array","ChatCompletionAudioParam","Reasoning","Responses","ResponseCreateParams","AzureOpenAIInput","Promise","AzureOpenAIChatInput","ChatOpenAIResponseFormatJSONSchema","Omit","ChatOpenAIReasoningSummary","ResponseReasoningItem","Summary","ChatOpenAIResponseFormat","ResponseFormatConfiguration"],"sources":["../src/types.d.ts"],"sourcesContent":["import type { OpenAI as OpenAIClient } from \"openai\";\nimport type { ResponseFormatText, ResponseFormatJSONObject, ResponseFormatJSONSchema, ResponseFormatTextPython, ResponseFormatTextGrammar } from \"openai/resources/shared\";\nimport { TiktokenModel } from \"js-tiktoken/lite\";\nimport type { BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { InteropZodObject } from \"@langchain/core/utils/types\";\n// reexport this type from the included package so we can easily override and extend it if needed in the future\n// also makes it easier for folks to import this type without digging around into the dependent packages\nexport type { TiktokenModel };\n/**\n * @see https://platform.openai.com/docs/models\n */\nexport type OpenAIChatModelId = OpenAIClient.ChatModel | (string & NonNullable<unknown>);\nexport type OpenAIVerbosityParam = \"low\" | \"medium\" | \"high\" | null;\nexport declare interface OpenAIBaseInput {\n    /** Sampling temperature to use */\n    temperature: number;\n    /**\n     * Maximum number of tokens to generate in the completion. -1 returns as many\n     * tokens as possible given the prompt and the model's maximum context size.\n     */\n    maxTokens?: number;\n    /**\n     * Maximum number of tokens to generate in the completion. -1 returns as many\n     * tokens as possible given the prompt and the model's maximum context size.\n     * Alias for `maxTokens` for reasoning models.\n     */\n    maxCompletionTokens?: number;\n    /** Total probability mass of tokens to consider at each step */\n    topP: number;\n    /** Penalizes repeated tokens according to frequency */\n    frequencyPenalty: number;\n    /** Penalizes repeated tokens */\n    presencePenalty: number;\n    /** Number of completions to generate for each prompt */\n    n: number;\n    /** Dictionary used to adjust the probability of specific tokens being generated */\n    logitBias?: Record<string, number>;\n    /** Unique string identifier representing your end-user, which can help OpenAI to monitor and detect abuse. */\n    user?: string;\n    /** Whether to stream the results or not. Enabling disables tokenUsage reporting */\n    streaming: boolean;\n    /**\n     * Whether or not to include token usage data in streamed chunks.\n     * @default true\n     */\n    streamUsage?: boolean;\n    /**\n     * Model name to use\n     * Alias for `model`\n     * @deprecated Use \"model\" instead.\n     */\n    modelName: string;\n    /** Model name to use */\n    model: OpenAIChatModelId;\n    /** Holds any additional parameters that are valid to pass to {@link\n     * https://platform.openai.com/docs/api-reference/completions/create |\n     * `openai.createCompletion`} that are not explicitly specified on this class.\n     */\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    modelKwargs?: Record<string, any>;\n    /**\n     * List of stop words to use when generating\n     * Alias for `stopSequences`\n     */\n    stop?: string[];\n    /** List of stop words to use when generating */\n    stopSequences?: string[];\n    /**\n     * Timeout to use when making requests to OpenAI.\n     */\n    timeout?: number;\n    /**\n     * API key to use when making requests to OpenAI. Defaults to the value of\n     * `OPENAI_API_KEY` environment variable.\n     * Alias for `apiKey`\n     */\n    openAIApiKey?: string;\n    /**\n     * API key to use when making requests to OpenAI. Defaults to the value of\n     * `OPENAI_API_KEY` environment variable.\n     */\n    apiKey?: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n}\nexport type OpenAICoreRequestOptions = OpenAIClient.RequestOptions;\nexport interface OpenAICallOptions extends BaseLanguageModelCallOptions {\n    /**\n     * Additional options to pass to the underlying axios request.\n     */\n    options?: OpenAICoreRequestOptions;\n}\n/**\n * Input to OpenAI class.\n */\nexport declare interface OpenAIInput extends OpenAIBaseInput {\n    /** Generates `bestOf` completions server side and returns the \"best\" */\n    bestOf?: number;\n    /** Batch size to use when passing multiple documents to generate */\n    batchSize: number;\n}\nexport interface OpenAIChatInput extends OpenAIBaseInput {\n    /**\n     * Whether to return log probabilities of the output tokens or not.\n     * If true, returns the log probabilities of each output token returned in the content of message.\n     */\n    logprobs?: boolean;\n    /**\n     * An integer between 0 and 5 specifying the number of most likely tokens to return at each token position,\n     * each with an associated log probability. logprobs must be set to true if this parameter is used.\n     */\n    topLogprobs?: number;\n    /** ChatGPT messages to pass as a prefix to the prompt */\n    prefixMessages?: OpenAIClient.Chat.ChatCompletionMessageParam[];\n    /**\n     * Whether to include the raw OpenAI response in the output message's \"additional_kwargs\" field.\n     * Currently in experimental beta.\n     */\n    __includeRawResponse?: boolean;\n    /**\n     * Whether the model supports the `strict` argument when passing in tools.\n     * If `undefined` the `strict` argument will not be passed to OpenAI.\n     */\n    supportsStrictToolCalling?: boolean;\n    /**\n     * Output types that you would like the model to generate for this request. Most\n     * models are capable of generating text, which is the default:\n     *\n     * `[\"text\"]`\n     *\n     * The `gpt-4o-audio-preview` model can also be used to\n     * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n     * this model generate both text and audio responses, you can use:\n     *\n     * `[\"text\", \"audio\"]`\n     */\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    /**\n     * Parameters for audio output. Required when audio output is requested with\n     * `modalities: [\"audio\"]`.\n     * [Learn more](https://platform.openai.com/docs/guides/audio).\n     */\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    /**\n     * Options for reasoning models.\n     *\n     * Note that some options, like reasoning summaries, are only available when using the responses\n     * API. This option is ignored when not using a reasoning model.\n     */\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Should be set to `true` in tenancies with Zero Data Retention\n     * @see https://platform.openai.com/docs/guides/your-data\n     *\n     * @default false\n     */\n    zdrEnabled?: boolean;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Responses.ResponseCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates. Replaces the `user` field.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey?: string;\n}\nexport interface AzureOpenAIInput {\n    /**\n     * API version to use when making requests to Azure OpenAI.\n     */\n    azureOpenAIApiVersion?: string;\n    /**\n     * API key to use when making requests to Azure OpenAI.\n     */\n    azureOpenAIApiKey?: string;\n    /**\n     * Azure OpenAI API instance name to use when making requests to Azure OpenAI.\n     * this is the name of the instance you created in the Azure portal.\n     * e.g. \"my-openai-instance\"\n     * this will be used in the endpoint URL: https://my-openai-instance.openai.azure.com/openai/deployments/{DeploymentName}/\n     */\n    azureOpenAIApiInstanceName?: string;\n    /**\n     * Azure OpenAI API deployment name to use for completions when making requests to Azure OpenAI.\n     * This is the name of the deployment you created in the Azure portal.\n     * e.g. \"my-openai-deployment\"\n     * this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n     */\n    azureOpenAIApiDeploymentName?: string;\n    /**\n     * Azure OpenAI API deployment name to use for embedding when making requests to Azure OpenAI.\n     * This is the name of the deployment you created in the Azure portal.\n     * This will fallback to azureOpenAIApiDeploymentName if not provided.\n     * e.g. \"my-openai-deployment\"\n     * this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n     */\n    azureOpenAIApiEmbeddingsDeploymentName?: string;\n    /**\n     * Azure OpenAI API deployment name to use for completions when making requests to Azure OpenAI.\n     * Completions are only available for gpt-3.5-turbo and text-davinci-003 deployments.\n     * This is the name of the deployment you created in the Azure portal.\n     * This will fallback to azureOpenAIApiDeploymentName if not provided.\n     * e.g. \"my-openai-deployment\"\n     * this will be used in the endpoint URL: https://{InstanceName}.openai.azure.com/openai/deployments/my-openai-deployment/\n     */\n    azureOpenAIApiCompletionsDeploymentName?: string;\n    /**\n     * Custom base url for Azure OpenAI API. This is useful in case you have a deployment in another region.\n     * e.g. setting this value to \"https://westeurope.api.cognitive.microsoft.com/openai/deployments\"\n     * will be result in the endpoint URL: https://westeurope.api.cognitive.microsoft.com/openai/deployments/{DeploymentName}/\n     */\n    azureOpenAIBasePath?: string;\n    /**\n     * Custom endpoint for Azure OpenAI API. This is useful in case you have a deployment in another region.\n     * e.g. setting this value to \"https://westeurope.api.cognitive.microsoft.com/\"\n     * will be result in the endpoint URL: https://westeurope.api.cognitive.microsoft.com/openai/deployments/{DeploymentName}/\n     */\n    azureOpenAIEndpoint?: string;\n    /**\n     * A function that returns an access token for Microsoft Entra (formerly known as Azure Active Directory),\n     * which will be invoked on every request.\n     */\n    azureADTokenProvider?: () => Promise<string>;\n}\nexport interface AzureOpenAIChatInput extends OpenAIChatInput, AzureOpenAIInput {\n    openAIApiKey?: string;\n    openAIApiVersion?: string;\n    openAIBasePath?: string;\n    deploymentName?: string;\n}\ntype ChatOpenAIResponseFormatJSONSchema = Omit<ResponseFormatJSONSchema, \"json_schema\"> & {\n    json_schema: Omit<ResponseFormatJSONSchema[\"json_schema\"], \"schema\"> & {\n        /**\n         * The schema for the response format, described as a JSON Schema object\n         * or a Zod object.\n         */\n        // eslint-disable-next-line @typescript-eslint/no-explicit-any\n        schema: Record<string, any> | InteropZodObject;\n    };\n};\n/**\n * The summary of a model's reasoning step.\n */\nexport type ChatOpenAIReasoningSummary = Omit<OpenAIClient.Responses.ResponseReasoningItem, \"summary\"> & {\n    /**\n     * The summary of the reasoning step. The index field will be populated if the response was\n     * streamed. This allows LangChain to recompose the reasoning summary output correctly when the\n     * AIMessage is used as an input for future generation requests.\n     */\n    summary: Array<OpenAIClient.Responses.ResponseReasoningItem.Summary & {\n        index?: number;\n    }>;\n};\nexport type ChatOpenAIResponseFormat = ResponseFormatText | ResponseFormatJSONObject | ResponseFormatTextGrammar | ResponseFormatTextPython | ChatOpenAIResponseFormatJSONSchema;\nexport type ResponseFormatConfiguration = ResponseFormatText | ResponseFormatJSONObject | ResponseFormatJSONSchema;\n"],"mappings":";;;;;;;;;;;AAW8E,KAAlEU,iBAAAA,GAAoBT,QAAAA,CAAaU,SAAiC,GAAA,CAAA,MAAA,GAAXC,WAAW,CAAA,OAAA,CAAA,CAAA;AAClEC,KAAAA,oBAAAA,GAAoB,KAAA,GAAA,QAAA,GAAA,MAAA,GAAA,IAAA;AACPC,kBAAAA,eAAAA,CAAe;EAAA;EAAA,WAuBxBC,EAAAA,MAAAA;EAAM;;;AAiDc;EAExBC,SAAAA,CAAAA,EAAAA,MAAAA;EACKE;;;;AAAsD;EAS9CC,mBAAW,CAAA,EAAA,MAASL;EAM5BM;EAAe,IAAA,EAAA,MAAA;EAAA;EAYiC,gBAuB7BC,EAAKE,MAAAA;EAAsB;EAAzC,eAMGF,EAAKI,MAAAA;EAAwB;EAOhB,CAAA,EAYnBxB,MAAAA;EAA2C;EA5DN,SAAA,CAAA,EAnExCc,MAmEwC,CAAA,MAAA,EAAA,MAAA,CAAA;EAoEvCc;EA0DAE,IAAAA,CAAAA,EAAAA,MAAAA;EAAoB;EAAA,SAASX,EAAAA,OAAAA;EAAe;AAAkB;AAK9E;;EACsC,WAAQhB,CAAAA,EAAAA,OAAAA;EAAwB;;;;;EAOjB,SAAA,EAAA,MAAA;EAM1C8B;EAA0B,KAAA,EAnM3BxB,iBAmM2B;EAAA;;;;EAMpB;EAIN2B,WAAAA,CAAAA,EAvMMtB,MAuMNsB,CAAAA,MAAwB,EAAA,GAAA,CAAA;EAAA;;;;EAA4E,IAAGhC,CAAAA,EAAAA,MAAAA,EAAAA;EAAwB;EAAqC,aAAA,CAAA,EAAA,MAAA,EAAA;EACpKiC;;;EAAgD,OAAGnC,CAAAA,EAAAA,MAAAA;EAAwB;AAA2B;;;;;;;;;;;;;cA9KlGU;;KAEJG,wBAAAA,GAA2Bf,QAAAA,CAAagB;UACnCC,iBAAAA,SAA0BV;;;;YAI7BQ;;;;;kBAKWG,WAAAA,SAAoBL;;;;;;UAM5BM,eAAAA,SAAwBN;;;;;;;;;;;;mBAYpBb,QAAAA,CAAaoB,IAAAA,CAAKC;;;;;;;;;;;;;;;;;;;;;;;eAuBtBE,MAAMvB,QAAAA,CAAaoB,IAAAA,CAAKE;;;;;;UAM7BtB,QAAAA,CAAaoB,IAAAA,CAAKI;;;;;;;cAOdxB,QAAAA,CAAayB;;;;;;;;;;;;iBAYVzB,QAAAA,CAAa0B,SAAAA,CAAUC;;;;;;;;UAQzBC,gBAAAA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;+BAwDgBC;;UAEhBC,oBAAAA,SAA6BX,iBAAiBS;;;;;;KAM1DG,kCAAAA,GAAqCC,KAAK7B;eAC9B6B,KAAK7B;;;;;;YAMNW,sBAAsBN;;;;;;KAM1ByB,0BAAAA,GAA6BD,KAAKhC,QAAAA,CAAa0B,SAAAA,CAAUQ;;;;;;WAMxDX,MAAMvB,QAAAA,CAAa0B,SAAAA,CAAUQ,qBAAAA,CAAsBC;;;;KAIpDC,wBAAAA,GAA2BnC,qBAAqBC,2BAA2BG,4BAA4BD,2BAA2B2B;KAClIM,2BAAAA,GAA8BpC,qBAAqBC,2BAA2BC"}