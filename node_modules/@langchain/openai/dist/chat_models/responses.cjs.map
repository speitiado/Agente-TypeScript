{"version":3,"file":"responses.cjs","names":["BaseChatOpenAI","options?: this[\"ParsedCallOptions\"]","strict: boolean | undefined","params: ChatResponsesInvocationParams","isBuiltInToolChoice","formatToOpenAIToolChoice","messages: BaseMessage[]","options: this[\"ParsedCallOptions\"]","finalChunk: ChatGenerationChunk | undefined","runManager?: CallbackManagerForLLMRun","request: OpenAIClient.Responses.ResponseCreateParams","requestOptions?: OpenAIClient.RequestOptions","wrapOpenAIClientError","response: ResponsesCreateInvoke | ResponsesParseInvoke","messageId: string | undefined","content: MessageContent","tool_calls: ToolCall[]","invalid_tool_calls: InvalidToolCall[]","response_metadata: Record<string, unknown>","additional_kwargs: {\n      [key: string]: unknown;\n      refusal?: string;\n      reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n      tool_outputs?: unknown[];\n      parsed?: unknown;\n      [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n    }","e: unknown","errMessage: string | undefined","parseCustomToolCall","AIMessage","_convertOpenAIResponsesUsageToLangChainUsage","chunk: OpenAIClient.Responses.ResponseStreamEvent","content: Record<string, unknown>[]","generationInfo: Record<string, unknown>","usage_metadata: UsageMetadata | undefined","tool_call_chunks: ToolCallChunk[]","additional_kwargs: {\n      [key: string]: unknown;\n      reasoning?: Partial<ChatOpenAIReasoningSummary>;\n      tool_outputs?: unknown[];\n    }","id: string | undefined","summary: ChatOpenAIReasoningSummary[\"summary\"] | undefined","ChatGenerationChunk","AIMessageChunk","_convertToResponsesMessageFromV1","messageToOpenAIRole","isReasoningModel","input: ResponsesInputItem[]","iife","isCustomToolCall","fallthroughCallTypes: ResponsesInputItem[\"type\"][]","messages: ResponsesInputItem[]","messages","completionsApiContentBlockConverter","reasoning: ChatOpenAIReasoningSummary","tools: ChatOpenAIToolType[]","fields: { stream?: boolean; strict?: boolean }","reducedTools: ResponsesTool[]","isBuiltInTool","isCustomTool","isOpenAICustomTool","convertCompletionsCustomTool"],"sources":["../../src/chat_models/responses.ts"],"sourcesContent":["import { OpenAI as OpenAIClient } from \"openai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  AIMessage,\n  AIMessageChunk,\n  type BaseMessage,\n  isAIMessage,\n  type UsageMetadata,\n  type BaseMessageFields,\n  type MessageContent,\n  type InvalidToolCall,\n  MessageContentImageUrl,\n  isDataContentBlock,\n  convertToProviderContentBlock,\n} from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatResult } from \"@langchain/core/outputs\";\nimport { isOpenAITool as isOpenAIFunctionTool } from \"@langchain/core/language_models/base\";\nimport {\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport type {\n  ToolCall,\n  ToolCallChunk,\n  ToolMessage,\n} from \"@langchain/core/messages/tool\";\nimport { ResponseInputMessageContentList } from \"openai/resources/responses/responses.js\";\nimport { ChatOpenAIReasoningSummary, OpenAIVerbosityParam } from \"../types.js\";\nimport { wrapOpenAIClientError } from \"../utils/client.js\";\nimport {\n  formatToOpenAIToolChoice,\n  _convertToOpenAITool,\n  ChatOpenAIToolType,\n  convertCompletionsCustomTool,\n  isBuiltInTool,\n  isBuiltInToolChoice,\n  isCustomTool,\n  isCustomToolCall,\n  isOpenAICustomTool,\n  parseCustomToolCall,\n  ResponsesTool,\n} from \"../utils/tools.js\";\nimport { _convertOpenAIResponsesUsageToLangChainUsage } from \"../utils/output.js\";\nimport {\n  _convertMessagesToOpenAIParams,\n  completionsApiContentBlockConverter,\n} from \"../utils/message_inputs.js\";\nimport {\n  _convertToResponsesMessageFromV1,\n  ResponsesInputItem,\n} from \"../utils/standard.js\";\nimport { iife, isReasoningModel, messageToOpenAIRole } from \"../utils/misc.js\";\nimport { BaseChatOpenAI, BaseChatOpenAICallOptions } from \"./base.js\";\n\nconst _FUNCTION_CALL_IDS_MAP_KEY = \"__openai_function_call_ids__\";\n\nexport interface ChatOpenAIResponsesCallOptions\n  extends BaseChatOpenAICallOptions {\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data.\n   */\n  text?: OpenAIClient.Responses.ResponseCreateParams[\"text\"];\n\n  /**\n   * The truncation strategy to use for the model response.\n   */\n  truncation?: OpenAIClient.Responses.ResponseCreateParams[\"truncation\"];\n\n  /**\n   * Specify additional output data to include in the model response.\n   */\n  include?: OpenAIClient.Responses.ResponseCreateParams[\"include\"];\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create multi-turn\n   * conversations.\n   */\n  previous_response_id?: OpenAIClient.Responses.ResponseCreateParams[\"previous_response_id\"];\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n}\n\ntype ChatResponsesInvocationParams = Omit<\n  OpenAIClient.Responses.ResponseCreateParams,\n  \"input\"\n>;\n\ntype ExcludeController<T> = T extends { controller: unknown } ? never : T;\n\ntype ResponsesCreate = OpenAIClient.Responses[\"create\"];\ntype ResponsesParse = OpenAIClient.Responses[\"parse\"];\n\ntype ResponsesCreateInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesCreate>>\n>;\ntype ResponsesParseInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesParse>>\n>;\n\n/**\n * OpenAI Responses API implementation.\n *\n * Will be exported in a later version of @langchain/openai.\n *\n * @internal\n */\nexport class ChatOpenAIResponses<\n  CallOptions extends ChatOpenAIResponsesCallOptions = ChatOpenAIResponsesCallOptions\n> extends BaseChatOpenAI<CallOptions> {\n  override invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): ChatResponsesInvocationParams {\n    let strict: boolean | undefined;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n\n    const params: ChatResponsesInvocationParams = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      user: this.user,\n\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      previous_response_id: options?.previous_response_id,\n      truncation: options?.truncation,\n      include: options?.include,\n      tools: options?.tools?.length\n        ? this._reduceChatOpenAITools(options.tools, {\n            stream: this.streaming,\n            strict,\n          })\n        : undefined,\n      tool_choice: isBuiltInToolChoice(options?.tool_choice)\n        ? options?.tool_choice\n        : (() => {\n            const formatted = formatToOpenAIToolChoice(options?.tool_choice);\n            if (typeof formatted === \"object\" && \"type\" in formatted) {\n              if (formatted.type === \"function\") {\n                return { type: \"function\", name: formatted.function.name };\n              } else if (formatted.type === \"allowed_tools\") {\n                return {\n                  type: \"allowed_tools\",\n                  mode: formatted.allowed_tools.mode,\n                  tools: formatted.allowed_tools.tools,\n                };\n              } else if (formatted.type === \"custom\") {\n                return {\n                  type: \"custom\",\n                  name: formatted.custom.name,\n                };\n              }\n            }\n            return undefined;\n          })(),\n      text: (() => {\n        if (options?.text) return options.text;\n        const format = this._getResponseFormat(options?.response_format);\n        if (format?.type === \"json_schema\") {\n          if (format.json_schema.schema != null) {\n            return {\n              format: {\n                type: \"json_schema\",\n                schema: format.json_schema.schema,\n                description: format.json_schema.description,\n                name: format.json_schema.name,\n                strict: format.json_schema.strict,\n              },\n              verbosity: options?.verbosity,\n            };\n          }\n          return undefined;\n        }\n        return { format, verbosity: options?.verbosity };\n      })(),\n      parallel_tool_calls: options?.parallel_tool_calls,\n      max_output_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n      prompt_cache_key: options?.promptCacheKey ?? this.promptCacheKey,\n      ...(this.zdrEnabled ? { store: false } : {}),\n      ...this.modelKwargs,\n    };\n\n    const reasoning = this._getReasoningParams(options);\n\n    if (reasoning !== undefined) {\n      params.reasoning = reasoning;\n    }\n\n    return params;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"]\n  ): Promise<ChatResult> {\n    const invocationParams = this.invocationParams(options);\n    if (invocationParams.stream) {\n      const stream = this._streamResponseChunks(messages, options);\n      let finalChunk: ChatGenerationChunk | undefined;\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata,\n        };\n        finalChunk = finalChunk?.concat(chunk) ?? chunk;\n      }\n\n      return {\n        generations: finalChunk ? [finalChunk] : [],\n        llmOutput: {\n          estimatedTokenUsage: (finalChunk?.message as AIMessage | undefined)\n            ?.usage_metadata,\n        },\n      };\n    } else {\n      const input = this._convertMessagesToResponsesParams(messages);\n      const data = await this.completionWithRetry(\n        {\n          input,\n          ...invocationParams,\n          stream: false,\n        },\n        { signal: options?.signal, ...options?.options }\n      );\n\n      return {\n        generations: [\n          {\n            text: data.output_text,\n            message: this._convertResponsesMessageToBaseMessage(data),\n          },\n        ],\n        llmOutput: {\n          id: data.id,\n          estimatedTokenUsage: data.usage\n            ? {\n                promptTokens: data.usage.input_tokens,\n                completionTokens: data.usage.output_tokens,\n                totalTokens: data.usage.total_tokens,\n              }\n            : undefined,\n        },\n      };\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const streamIterable = await this.completionWithRetry(\n      {\n        ...this.invocationParams(options),\n        input: this._convertMessagesToResponsesParams(messages),\n        stream: true,\n      },\n      options\n    );\n\n    for await (const data of streamIterable) {\n      const chunk = this._convertResponsesDeltaToBaseMessageChunk(data);\n      if (chunk == null) continue;\n      yield chunk;\n      await runManager?.handleLLMNewToken(\n        chunk.text || \"\",\n        {\n          prompt: options.promptIndex ?? 0,\n          completion: 0,\n        },\n        undefined,\n        undefined,\n        undefined,\n        { chunk }\n      );\n    }\n  }\n\n  /**\n   * Calls the Responses API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsNonStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<OpenAIClient.Responses.Response>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParams,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<\n    | AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>\n    | OpenAIClient.Responses.Response\n  > {\n    return this.caller.call(async () => {\n      const clientOptions = this._getClientOptions(requestOptions);\n      try {\n        // use parse if dealing with json_schema\n        if (request.text?.format?.type === \"json_schema\" && !request.stream) {\n          return await this.client.responses.parse(request, clientOptions);\n        }\n        return await this.client.responses.create(request, clientOptions);\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /** @internal */\n  protected _convertResponsesMessageToBaseMessage(\n    response: ResponsesCreateInvoke | ResponsesParseInvoke\n  ): BaseMessage {\n    if (response.error) {\n      // TODO: add support for `addLangChainErrorFields`\n      const error = new Error(response.error.message);\n      error.name = response.error.code;\n      throw error;\n    }\n\n    let messageId: string | undefined;\n    const content: MessageContent = [];\n    const tool_calls: ToolCall[] = [];\n    const invalid_tool_calls: InvalidToolCall[] = [];\n    const response_metadata: Record<string, unknown> = {\n      model_provider: \"openai\",\n      model: response.model,\n      created_at: response.created_at,\n      id: response.id,\n      incomplete_details: response.incomplete_details,\n      metadata: response.metadata,\n      object: response.object,\n      status: response.status,\n      user: response.user,\n      service_tier: response.service_tier,\n      // for compatibility with chat completion calls.\n      model_name: response.model,\n    };\n\n    const additional_kwargs: {\n      [key: string]: unknown;\n      refusal?: string;\n      reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n      tool_outputs?: unknown[];\n      parsed?: unknown;\n      [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n    } = {};\n\n    for (const item of response.output) {\n      if (item.type === \"message\") {\n        messageId = item.id;\n        content.push(\n          ...item.content.flatMap((part) => {\n            if (part.type === \"output_text\") {\n              if (\"parsed\" in part && part.parsed != null) {\n                additional_kwargs.parsed = part.parsed;\n              }\n              return {\n                type: \"text\",\n                text: part.text,\n                annotations: part.annotations,\n              };\n            }\n\n            if (part.type === \"refusal\") {\n              additional_kwargs.refusal = part.refusal;\n              return [];\n            }\n\n            return part;\n          })\n        );\n      } else if (item.type === \"function_call\") {\n        const fnAdapter = {\n          function: { name: item.name, arguments: item.arguments },\n          id: item.call_id,\n        };\n\n        try {\n          tool_calls.push(parseToolCall(fnAdapter, { returnId: true }));\n        } catch (e: unknown) {\n          let errMessage: string | undefined;\n          if (\n            typeof e === \"object\" &&\n            e != null &&\n            \"message\" in e &&\n            typeof e.message === \"string\"\n          ) {\n            errMessage = e.message;\n          }\n          invalid_tool_calls.push(makeInvalidToolCall(fnAdapter, errMessage));\n        }\n\n        additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] ??= {};\n        if (item.id) {\n          additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY][item.call_id] = item.id;\n        }\n      } else if (item.type === \"reasoning\") {\n        additional_kwargs.reasoning = item;\n      } else if (item.type === \"custom_tool_call\") {\n        const parsed = parseCustomToolCall(item);\n        if (parsed) {\n          tool_calls.push(parsed);\n        } else {\n          invalid_tool_calls.push(\n            makeInvalidToolCall(item, \"Malformed custom tool call\")\n          );\n        }\n      } else {\n        additional_kwargs.tool_outputs ??= [];\n        additional_kwargs.tool_outputs.push(item);\n      }\n    }\n\n    return new AIMessage({\n      id: messageId,\n      content,\n      tool_calls,\n      invalid_tool_calls,\n      usage_metadata: _convertOpenAIResponsesUsageToLangChainUsage(\n        response.usage\n      ),\n      additional_kwargs,\n      response_metadata,\n    });\n  }\n\n  /** @internal */\n  protected _convertResponsesDeltaToBaseMessageChunk(\n    chunk: OpenAIClient.Responses.ResponseStreamEvent\n  ) {\n    const content: Record<string, unknown>[] = [];\n    let generationInfo: Record<string, unknown> = {};\n    let usage_metadata: UsageMetadata | undefined;\n    const tool_call_chunks: ToolCallChunk[] = [];\n    const response_metadata: Record<string, unknown> = {\n      model_provider: \"openai\",\n    };\n    const additional_kwargs: {\n      [key: string]: unknown;\n      reasoning?: Partial<ChatOpenAIReasoningSummary>;\n      tool_outputs?: unknown[];\n    } = {};\n    let id: string | undefined;\n    if (chunk.type === \"response.output_text.delta\") {\n      content.push({\n        type: \"text\",\n        text: chunk.delta,\n        index: chunk.content_index,\n      });\n    } else if (chunk.type === \"response.output_text.annotation.added\") {\n      content.push({\n        type: \"text\",\n        text: \"\",\n        annotations: [chunk.annotation],\n        index: chunk.content_index,\n      });\n    } else if (\n      chunk.type === \"response.output_item.added\" &&\n      chunk.item.type === \"message\"\n    ) {\n      id = chunk.item.id;\n    } else if (\n      chunk.type === \"response.output_item.added\" &&\n      chunk.item.type === \"function_call\"\n    ) {\n      tool_call_chunks.push({\n        type: \"tool_call_chunk\",\n        name: chunk.item.name,\n        args: chunk.item.arguments,\n        id: chunk.item.call_id,\n        index: chunk.output_index,\n      });\n\n      additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] = {\n        [chunk.item.call_id]: chunk.item.id,\n      };\n    } else if (\n      chunk.type === \"response.output_item.done\" &&\n      [\n        \"web_search_call\",\n        \"file_search_call\",\n        \"computer_call\",\n        \"code_interpreter_call\",\n        \"mcp_call\",\n        \"mcp_list_tools\",\n        \"mcp_approval_request\",\n        \"image_generation_call\",\n        \"custom_tool_call\",\n      ].includes(chunk.item.type)\n    ) {\n      additional_kwargs.tool_outputs = [chunk.item];\n    } else if (chunk.type === \"response.created\") {\n      response_metadata.id = chunk.response.id;\n      response_metadata.model_name = chunk.response.model;\n      response_metadata.model = chunk.response.model;\n    } else if (chunk.type === \"response.completed\") {\n      const msg = this._convertResponsesMessageToBaseMessage(chunk.response);\n\n      usage_metadata = _convertOpenAIResponsesUsageToLangChainUsage(\n        chunk.response.usage\n      );\n\n      if (chunk.response.text?.format?.type === \"json_schema\") {\n        additional_kwargs.parsed ??= JSON.parse(msg.text);\n      }\n      for (const [key, value] of Object.entries(chunk.response)) {\n        if (key !== \"id\") response_metadata[key] = value;\n      }\n    } else if (\n      chunk.type === \"response.function_call_arguments.delta\" ||\n      chunk.type === \"response.custom_tool_call_input.delta\"\n    ) {\n      tool_call_chunks.push({\n        type: \"tool_call_chunk\",\n        args: chunk.delta,\n        index: chunk.output_index,\n      });\n    } else if (\n      chunk.type === \"response.web_search_call.completed\" ||\n      chunk.type === \"response.file_search_call.completed\"\n    ) {\n      generationInfo = {\n        tool_outputs: {\n          id: chunk.item_id,\n          type: chunk.type.replace(\"response.\", \"\").replace(\".completed\", \"\"),\n          status: \"completed\",\n        },\n      };\n    } else if (chunk.type === \"response.refusal.done\") {\n      additional_kwargs.refusal = chunk.refusal;\n    } else if (\n      chunk.type === \"response.output_item.added\" &&\n      \"item\" in chunk &&\n      chunk.item.type === \"reasoning\"\n    ) {\n      const summary: ChatOpenAIReasoningSummary[\"summary\"] | undefined = chunk\n        .item.summary\n        ? chunk.item.summary.map((s, index) => ({\n            ...s,\n            index,\n          }))\n        : undefined;\n\n      additional_kwargs.reasoning = {\n        // We only capture ID in the first chunk or else the concatenated result of all chunks will\n        // have an ID field that is repeated once per chunk. There is special handling for the `type`\n        // field that prevents this, however.\n        id: chunk.item.id,\n        type: chunk.item.type,\n        ...(summary ? { summary } : {}),\n      };\n    } else if (chunk.type === \"response.reasoning_summary_part.added\") {\n      additional_kwargs.reasoning = {\n        type: \"reasoning\",\n        summary: [{ ...chunk.part, index: chunk.summary_index }],\n      };\n    } else if (chunk.type === \"response.reasoning_summary_text.delta\") {\n      additional_kwargs.reasoning = {\n        type: \"reasoning\",\n        summary: [\n          {\n            text: chunk.delta,\n            type: \"summary_text\",\n            index: chunk.summary_index,\n          },\n        ],\n      };\n    } else if (chunk.type === \"response.image_generation_call.partial_image\") {\n      // noop/fixme: retaining partial images in a message chunk means that _all_\n      // partial images get kept in history, so we don't do anything here.\n      return null;\n    } else {\n      return null;\n    }\n\n    return new ChatGenerationChunk({\n      // Legacy reasons, `onLLMNewToken` should pulls this out\n      text: content.map((part) => part.text).join(\"\"),\n      message: new AIMessageChunk({\n        id,\n        content: content as MessageContent,\n        tool_call_chunks,\n        usage_metadata,\n        additional_kwargs,\n        response_metadata,\n      }),\n      generationInfo,\n    });\n  }\n\n  /** @internal */\n  protected _convertMessagesToResponsesParams(messages: BaseMessage[]) {\n    return messages.flatMap(\n      (lcMsg): ResponsesInputItem | ResponsesInputItem[] => {\n        const responseMetadata = lcMsg.response_metadata as\n          | Record<string, unknown>\n          | undefined;\n        if (responseMetadata?.output_version === \"v1\") {\n          return _convertToResponsesMessageFromV1(lcMsg);\n        }\n\n        const additional_kwargs = lcMsg.additional_kwargs as\n          | BaseMessageFields[\"additional_kwargs\"] & {\n              [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n              reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n              type?: string;\n              refusal?: string;\n            };\n\n        let role = messageToOpenAIRole(lcMsg);\n        if (role === \"system\" && isReasoningModel(this.model))\n          role = \"developer\";\n\n        if (role === \"function\") {\n          throw new Error(\n            \"Function messages are not supported in Responses API\"\n          );\n        }\n\n        if (role === \"tool\") {\n          const toolMessage = lcMsg as ToolMessage;\n\n          // Handle computer call output\n          if (additional_kwargs?.type === \"computer_call_output\") {\n            const output = (() => {\n              if (typeof toolMessage.content === \"string\") {\n                return {\n                  type: \"computer_screenshot\" as const,\n                  image_url: toolMessage.content,\n                };\n              }\n\n              if (Array.isArray(toolMessage.content)) {\n                const oaiScreenshot = toolMessage.content.find(\n                  (i) => i.type === \"computer_screenshot\"\n                ) as { type: \"computer_screenshot\"; image_url: string };\n\n                if (oaiScreenshot) return oaiScreenshot;\n\n                const lcImage = toolMessage.content.find(\n                  (i) => i.type === \"image_url\"\n                ) as MessageContentImageUrl;\n\n                if (lcImage) {\n                  return {\n                    type: \"computer_screenshot\" as const,\n                    image_url:\n                      typeof lcImage.image_url === \"string\"\n                        ? lcImage.image_url\n                        : lcImage.image_url.url,\n                  };\n                }\n              }\n\n              throw new Error(\"Invalid computer call output\");\n            })();\n\n            return {\n              type: \"computer_call_output\",\n              output,\n              call_id: toolMessage.tool_call_id,\n            };\n          }\n\n          // Handle custom tool output\n          if (toolMessage.additional_kwargs?.customTool) {\n            return {\n              type: \"custom_tool_call_output\",\n              call_id: toolMessage.tool_call_id,\n              output: toolMessage.content as string,\n            };\n          }\n\n          return {\n            type: \"function_call_output\",\n            call_id: toolMessage.tool_call_id,\n            id: toolMessage.id?.startsWith(\"fc_\") ? toolMessage.id : undefined,\n            output:\n              typeof toolMessage.content !== \"string\"\n                ? JSON.stringify(toolMessage.content)\n                : toolMessage.content,\n          };\n        }\n\n        if (role === \"assistant\") {\n          // if we have the original response items, just reuse them\n          if (\n            !this.zdrEnabled &&\n            responseMetadata?.output != null &&\n            Array.isArray(responseMetadata?.output) &&\n            responseMetadata?.output.length > 0 &&\n            responseMetadata?.output.every((item) => \"type\" in item)\n          ) {\n            return responseMetadata?.output;\n          }\n\n          // otherwise, try to reconstruct the response from what we have\n\n          const input: ResponsesInputItem[] = [];\n\n          // reasoning items\n          if (additional_kwargs?.reasoning && !this.zdrEnabled) {\n            const reasoningItem = this._convertReasoningSummary(\n              additional_kwargs.reasoning\n            );\n            input.push(reasoningItem);\n          }\n\n          // ai content\n          let { content } = lcMsg;\n          if (additional_kwargs?.refusal) {\n            if (typeof content === \"string\") {\n              content = [\n                { type: \"output_text\", text: content, annotations: [] },\n              ];\n            }\n            content = [\n              ...content,\n              { type: \"refusal\", refusal: additional_kwargs.refusal },\n            ];\n          }\n\n          if (typeof content === \"string\" || content.length > 0) {\n            input.push({\n              type: \"message\",\n              role: \"assistant\",\n              ...(lcMsg.id && !this.zdrEnabled && lcMsg.id.startsWith(\"msg_\")\n                ? { id: lcMsg.id }\n                : {}),\n              content: iife(() => {\n                if (typeof content === \"string\") {\n                  return content;\n                }\n                return content.flatMap((item) => {\n                  if (item.type === \"text\") {\n                    return {\n                      type: \"output_text\",\n                      text: item.text,\n                      annotations: item.annotations ?? [],\n                    };\n                  }\n\n                  if (item.type === \"output_text\" || item.type === \"refusal\") {\n                    return item;\n                  }\n\n                  return [];\n                });\n              }) as ResponseInputMessageContentList,\n            });\n          }\n\n          const functionCallIds =\n            additional_kwargs?.[_FUNCTION_CALL_IDS_MAP_KEY];\n\n          if (isAIMessage(lcMsg) && !!lcMsg.tool_calls?.length) {\n            input.push(\n              ...lcMsg.tool_calls.map((toolCall): ResponsesInputItem => {\n                if (isCustomToolCall(toolCall)) {\n                  return {\n                    type: \"custom_tool_call\",\n                    id: toolCall.call_id,\n                    call_id: toolCall.id ?? \"\",\n                    input: toolCall.args.input,\n                    name: toolCall.name,\n                  };\n                }\n                return {\n                  type: \"function_call\",\n                  name: toolCall.name,\n                  arguments: JSON.stringify(toolCall.args),\n                  call_id: toolCall.id!,\n                  ...(this.zdrEnabled\n                    ? { id: functionCallIds?.[toolCall.id!] }\n                    : {}),\n                };\n              })\n            );\n          } else if (additional_kwargs?.tool_calls) {\n            input.push(\n              ...additional_kwargs.tool_calls.map(\n                (toolCall): ResponsesInputItem => ({\n                  type: \"function_call\",\n                  name: toolCall.function.name,\n                  call_id: toolCall.id,\n                  arguments: toolCall.function.arguments,\n                  ...(this.zdrEnabled\n                    ? { id: functionCallIds?.[toolCall.id] }\n                    : {}),\n                })\n              )\n            );\n          }\n\n          const toolOutputs = (\n            responseMetadata?.output as Array<ResponsesInputItem>\n          )?.length\n            ? responseMetadata?.output\n            : additional_kwargs.tool_outputs;\n\n          const fallthroughCallTypes: ResponsesInputItem[\"type\"][] = [\n            \"computer_call\",\n            \"mcp_call\",\n            \"code_interpreter_call\",\n            \"image_generation_call\",\n          ];\n\n          if (toolOutputs != null) {\n            const castToolOutputs = toolOutputs as Array<ResponsesInputItem>;\n            const fallthroughCalls = castToolOutputs?.filter((item) =>\n              fallthroughCallTypes.includes(item.type)\n            );\n            if (fallthroughCalls.length > 0) input.push(...fallthroughCalls);\n          }\n\n          return input;\n        }\n\n        if (role === \"user\" || role === \"system\" || role === \"developer\") {\n          if (typeof lcMsg.content === \"string\") {\n            return { type: \"message\", role, content: lcMsg.content };\n          }\n\n          const messages: ResponsesInputItem[] = [];\n          const content = lcMsg.content.flatMap((item) => {\n            if (item.type === \"mcp_approval_response\") {\n              messages.push({\n                type: \"mcp_approval_response\",\n                approval_request_id: item.approval_request_id as string,\n                approve: item.approve as boolean,\n              });\n            }\n            if (isDataContentBlock(item)) {\n              return convertToProviderContentBlock(\n                item,\n                completionsApiContentBlockConverter\n              );\n            }\n            if (item.type === \"text\") {\n              return {\n                type: \"input_text\",\n                text: item.text,\n              };\n            }\n            if (item.type === \"image_url\") {\n              const imageUrl = iife(() => {\n                if (typeof item.image_url === \"string\") {\n                  return item.image_url;\n                } else if (\n                  typeof item.image_url === \"object\" &&\n                  item.image_url !== null &&\n                  \"url\" in item.image_url\n                ) {\n                  return item.image_url.url;\n                }\n                return undefined;\n              });\n              const detail = iife(() => {\n                if (typeof item.image_url === \"string\") {\n                  return \"auto\";\n                } else if (\n                  typeof item.image_url === \"object\" &&\n                  item.image_url !== null &&\n                  \"detail\" in item.image_url\n                ) {\n                  return item.image_url.detail;\n                }\n                return undefined;\n              });\n              return {\n                type: \"input_image\",\n                image_url: imageUrl,\n                detail,\n              };\n            }\n            if (\n              item.type === \"input_text\" ||\n              item.type === \"input_image\" ||\n              item.type === \"input_file\"\n            ) {\n              return item;\n            }\n            return [];\n          });\n\n          if (content.length > 0) {\n            messages.push({\n              type: \"message\",\n              role,\n              content: content as ResponseInputMessageContentList,\n            });\n          }\n          return messages;\n        }\n\n        console.warn(\n          `Unsupported role found when converting to OpenAI Responses API: ${role}`\n        );\n        return [];\n      }\n    );\n  }\n\n  /** @internal */\n  protected _convertReasoningSummary(\n    reasoning: ChatOpenAIReasoningSummary\n  ): OpenAIClient.Responses.ResponseReasoningItem {\n    // combine summary parts that have the the same index and then remove the indexes\n    const summary = (\n      reasoning.summary.length > 1\n        ? reasoning.summary.reduce(\n            (acc, curr) => {\n              const last = acc[acc.length - 1];\n\n              if (last!.index === curr.index) {\n                last!.text += curr.text;\n              } else {\n                acc.push(curr);\n              }\n              return acc;\n            },\n            [{ ...reasoning.summary[0] }]\n          )\n        : reasoning.summary\n    ).map((s) =>\n      Object.fromEntries(Object.entries(s).filter(([k]) => k !== \"index\"))\n    ) as OpenAIClient.Responses.ResponseReasoningItem.Summary[];\n\n    return {\n      ...reasoning,\n      summary,\n    } as OpenAIClient.Responses.ResponseReasoningItem;\n  }\n\n  /** @internal */\n  protected _reduceChatOpenAITools(\n    tools: ChatOpenAIToolType[],\n    fields: { stream?: boolean; strict?: boolean }\n  ): ResponsesTool[] {\n    const reducedTools: ResponsesTool[] = [];\n    for (const tool of tools) {\n      if (isBuiltInTool(tool)) {\n        if (tool.type === \"image_generation\" && fields?.stream) {\n          // OpenAI sends a 400 error if partial_images is not set and we want to stream.\n          // We also set it to 1 since we don't support partial images yet.\n          tool.partial_images = 1;\n        }\n        reducedTools.push(tool);\n      } else if (isCustomTool(tool)) {\n        const customToolData = tool.metadata.customTool;\n        reducedTools.push({\n          type: \"custom\",\n          name: customToolData.name,\n          description: customToolData.description,\n          format: customToolData.format,\n        } as ResponsesTool);\n      } else if (isOpenAIFunctionTool(tool)) {\n        reducedTools.push({\n          type: \"function\",\n          name: tool.function.name,\n          parameters: tool.function.parameters,\n          description: tool.function.description,\n          strict: fields?.strict ?? null,\n        });\n      } else if (isOpenAICustomTool(tool)) {\n        reducedTools.push(convertCompletionsCustomTool(tool));\n      }\n    }\n    return reducedTools;\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;AAsDA,MAAM,6BAA6B;;;;;;;;AAwDnC,IAAa,sBAAb,cAEUA,4BAA4B;CACpC,AAAS,iBACPC,SAC+B;EAC/B,IAAIC;AACJ,MAAI,SAAS,WAAW,QACtB,SAAS,QAAQ;WACR,KAAK,8BAA8B,QAC5C,SAAS,KAAK;EAGhB,MAAMC,SAAwC;GAC5C,OAAO,KAAK;GACZ,aAAa,KAAK;GAClB,OAAO,KAAK;GACZ,MAAM,KAAK;GAGX,QAAQ,KAAK;GACb,sBAAsB,SAAS;GAC/B,YAAY,SAAS;GACrB,SAAS,SAAS;GAClB,OAAO,SAAS,OAAO,SACnB,KAAK,uBAAuB,QAAQ,OAAO;IACzC,QAAQ,KAAK;IACb;GACD,EAAC,GACF;GACJ,aAAaC,kCAAoB,SAAS,YAAY,GAClD,SAAS,eACR,MAAM;IACL,MAAM,YAAYC,uCAAyB,SAAS,YAAY;AAChE,QAAI,OAAO,cAAc,YAAY,UAAU,WAC7C;SAAI,UAAU,SAAS,WACrB,QAAO;MAAE,MAAM;MAAY,MAAM,UAAU,SAAS;KAAM;cACjD,UAAU,SAAS,gBAC5B,QAAO;MACL,MAAM;MACN,MAAM,UAAU,cAAc;MAC9B,OAAO,UAAU,cAAc;KAChC;cACQ,UAAU,SAAS,SAC5B,QAAO;MACL,MAAM;MACN,MAAM,UAAU,OAAO;KACxB;IACF;AAEH,WAAO;GACR,IAAG;GACR,OAAO,MAAM;AACX,QAAI,SAAS,KAAM,QAAO,QAAQ;IAClC,MAAM,SAAS,KAAK,mBAAmB,SAAS,gBAAgB;AAChE,QAAI,QAAQ,SAAS,eAAe;AAClC,SAAI,OAAO,YAAY,UAAU,KAC/B,QAAO;MACL,QAAQ;OACN,MAAM;OACN,QAAQ,OAAO,YAAY;OAC3B,aAAa,OAAO,YAAY;OAChC,MAAM,OAAO,YAAY;OACzB,QAAQ,OAAO,YAAY;MAC5B;MACD,WAAW,SAAS;KACrB;AAEH,YAAO;IACR;AACD,WAAO;KAAE;KAAQ,WAAW,SAAS;IAAW;GACjD,IAAG;GACJ,qBAAqB,SAAS;GAC9B,mBAAmB,KAAK,cAAc,KAAK,SAAY,KAAK;GAC5D,kBAAkB,SAAS,kBAAkB,KAAK;GAClD,GAAI,KAAK,aAAa,EAAE,OAAO,MAAO,IAAG,CAAE;GAC3C,GAAG,KAAK;EACT;EAED,MAAM,YAAY,KAAK,oBAAoB,QAAQ;AAEnD,MAAI,cAAc,QAChB,OAAO,YAAY;AAGrB,SAAO;CACR;CAED,MAAM,UACJC,UACAC,SACqB;EACrB,MAAM,mBAAmB,KAAK,iBAAiB,QAAQ;AACvD,MAAI,iBAAiB,QAAQ;GAC3B,MAAM,SAAS,KAAK,sBAAsB,UAAU,QAAQ;GAC5D,IAAIC;AACJ,cAAW,MAAM,SAAS,QAAQ;IAChC,MAAM,QAAQ,oBAAoB;KAChC,GAAG,MAAM;KACT,GAAG,MAAM,QAAQ;IAClB;IACD,aAAa,YAAY,OAAO,MAAM,IAAI;GAC3C;AAED,UAAO;IACL,aAAa,aAAa,CAAC,UAAW,IAAG,CAAE;IAC3C,WAAW,EACT,sBAAsB,YAAY,UAC9B,eACL;GACF;EACF,OAAM;GACL,MAAM,QAAQ,KAAK,kCAAkC,SAAS;GAC9D,MAAM,OAAO,MAAM,KAAK,oBACtB;IACE;IACA,GAAG;IACH,QAAQ;GACT,GACD;IAAE,QAAQ,SAAS;IAAQ,GAAG,SAAS;GAAS,EACjD;AAED,UAAO;IACL,aAAa,CACX;KACE,MAAM,KAAK;KACX,SAAS,KAAK,sCAAsC,KAAK;IAC1D,CACF;IACD,WAAW;KACT,IAAI,KAAK;KACT,qBAAqB,KAAK,QACtB;MACE,cAAc,KAAK,MAAM;MACzB,kBAAkB,KAAK,MAAM;MAC7B,aAAa,KAAK,MAAM;KACzB,IACD;IACL;GACF;EACF;CACF;CAED,OAAO,sBACLF,UACAC,SACAE,YACqC;EACrC,MAAM,iBAAiB,MAAM,KAAK,oBAChC;GACE,GAAG,KAAK,iBAAiB,QAAQ;GACjC,OAAO,KAAK,kCAAkC,SAAS;GACvD,QAAQ;EACT,GACD,QACD;AAED,aAAW,MAAM,QAAQ,gBAAgB;GACvC,MAAM,QAAQ,KAAK,yCAAyC,KAAK;AACjE,OAAI,SAAS,KAAM;GACnB,MAAM;GACN,MAAM,YAAY,kBAChB,MAAM,QAAQ,IACd;IACE,QAAQ,QAAQ,eAAe;IAC/B,YAAY;GACb,GACD,QACA,QACA,QACA,EAAE,MAAO,EACV;EACF;CACF;CAkBD,MAAM,oBACJC,SACAC,gBAIA;AACA,SAAO,KAAK,OAAO,KAAK,YAAY;GAClC,MAAM,gBAAgB,KAAK,kBAAkB,eAAe;AAC5D,OAAI;AAEF,QAAI,QAAQ,MAAM,QAAQ,SAAS,iBAAiB,CAAC,QAAQ,OAC3D,QAAO,MAAM,KAAK,OAAO,UAAU,MAAM,SAAS,cAAc;AAElE,WAAO,MAAM,KAAK,OAAO,UAAU,OAAO,SAAS,cAAc;GAClE,SAAQ,GAAG;IACV,MAAM,QAAQC,qCAAsB,EAAE;AACtC,UAAM;GACP;EACF,EAAC;CACH;;CAGD,AAAU,sCACRC,UACa;AACb,MAAI,SAAS,OAAO;GAElB,MAAM,QAAQ,IAAI,MAAM,SAAS,MAAM;GACvC,MAAM,OAAO,SAAS,MAAM;AAC5B,SAAM;EACP;EAED,IAAIC;EACJ,MAAMC,UAA0B,CAAE;EAClC,MAAMC,aAAyB,CAAE;EACjC,MAAMC,qBAAwC,CAAE;EAChD,MAAMC,oBAA6C;GACjD,gBAAgB;GAChB,OAAO,SAAS;GAChB,YAAY,SAAS;GACrB,IAAI,SAAS;GACb,oBAAoB,SAAS;GAC7B,UAAU,SAAS;GACnB,QAAQ,SAAS;GACjB,QAAQ,SAAS;GACjB,MAAM,SAAS;GACf,cAAc,SAAS;GAEvB,YAAY,SAAS;EACtB;EAED,MAAMC,oBAOF,CAAE;AAEN,OAAK,MAAM,QAAQ,SAAS,OAC1B,KAAI,KAAK,SAAS,WAAW;GAC3B,YAAY,KAAK;GACjB,QAAQ,KACN,GAAG,KAAK,QAAQ,QAAQ,CAAC,SAAS;AAChC,QAAI,KAAK,SAAS,eAAe;AAC/B,SAAI,YAAY,QAAQ,KAAK,UAAU,MACrC,kBAAkB,SAAS,KAAK;AAElC,YAAO;MACL,MAAM;MACN,MAAM,KAAK;MACX,aAAa,KAAK;KACnB;IACF;AAED,QAAI,KAAK,SAAS,WAAW;KAC3B,kBAAkB,UAAU,KAAK;AACjC,YAAO,CAAE;IACV;AAED,WAAO;GACR,EAAC,CACH;EACF,WAAU,KAAK,SAAS,iBAAiB;GACxC,MAAM,YAAY;IAChB,UAAU;KAAE,MAAM,KAAK;KAAM,WAAW,KAAK;IAAW;IACxD,IAAI,KAAK;GACV;AAED,OAAI;IACF,WAAW,qEAAmB,WAAW,EAAE,UAAU,KAAM,EAAC,CAAC;GAC9D,SAAQC,GAAY;IACnB,IAAIC;AACJ,QACE,OAAO,MAAM,YACb,KAAK,QACL,aAAa,KACb,OAAO,EAAE,YAAY,UAErB,aAAa,EAAE;IAEjB,mBAAmB,2EAAyB,WAAW,WAAW,CAAC;GACpE;GAED,kBAAkB,gCAAgC,CAAE;AACpD,OAAI,KAAK,IACP,kBAAkB,4BAA4B,KAAK,WAAW,KAAK;EAEtE,WAAU,KAAK,SAAS,aACvB,kBAAkB,YAAY;WACrB,KAAK,SAAS,oBAAoB;GAC3C,MAAM,SAASC,kCAAoB,KAAK;AACxC,OAAI,QACF,WAAW,KAAK,OAAO;QAEvB,mBAAmB,2EACG,MAAM,6BAA6B,CACxD;EAEJ,OAAM;GACL,kBAAkB,iBAAiB,CAAE;GACrC,kBAAkB,aAAa,KAAK,KAAK;EAC1C;AAGH,SAAO,IAAIC,oCAAU;GACnB,IAAI;GACJ;GACA;GACA;GACA,gBAAgBC,4DACd,SAAS,MACV;GACD;GACA;EACD;CACF;;CAGD,AAAU,yCACRC,OACA;EACA,MAAMC,UAAqC,CAAE;EAC7C,IAAIC,iBAA0C,CAAE;EAChD,IAAIC;EACJ,MAAMC,mBAAoC,CAAE;EAC5C,MAAMX,oBAA6C,EACjD,gBAAgB,SACjB;EACD,MAAMY,oBAIF,CAAE;EACN,IAAIC;AACJ,MAAI,MAAM,SAAS,8BACjB,QAAQ,KAAK;GACX,MAAM;GACN,MAAM,MAAM;GACZ,OAAO,MAAM;EACd,EAAC;WACO,MAAM,SAAS,yCACxB,QAAQ,KAAK;GACX,MAAM;GACN,MAAM;GACN,aAAa,CAAC,MAAM,UAAW;GAC/B,OAAO,MAAM;EACd,EAAC;WAEF,MAAM,SAAS,gCACf,MAAM,KAAK,SAAS,WAEpB,KAAK,MAAM,KAAK;WAEhB,MAAM,SAAS,gCACf,MAAM,KAAK,SAAS,iBACpB;GACA,iBAAiB,KAAK;IACpB,MAAM;IACN,MAAM,MAAM,KAAK;IACjB,MAAM,MAAM,KAAK;IACjB,IAAI,MAAM,KAAK;IACf,OAAO,MAAM;GACd,EAAC;GAEF,kBAAkB,8BAA8B,GAC7C,MAAM,KAAK,UAAU,MAAM,KAAK,GAClC;EACF,WACC,MAAM,SAAS,+BACf;GACE;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;EACD,EAAC,SAAS,MAAM,KAAK,KAAK,EAE3B,kBAAkB,eAAe,CAAC,MAAM,IAAK;WACpC,MAAM,SAAS,oBAAoB;GAC5C,kBAAkB,KAAK,MAAM,SAAS;GACtC,kBAAkB,aAAa,MAAM,SAAS;GAC9C,kBAAkB,QAAQ,MAAM,SAAS;EAC1C,WAAU,MAAM,SAAS,sBAAsB;GAC9C,MAAM,MAAM,KAAK,sCAAsC,MAAM,SAAS;GAEtE,iBAAiBP,4DACf,MAAM,SAAS,MAChB;AAED,OAAI,MAAM,SAAS,MAAM,QAAQ,SAAS,eACxC,kBAAkB,WAAW,KAAK,MAAM,IAAI,KAAK;AAEnD,QAAK,MAAM,CAAC,KAAK,MAAM,IAAI,OAAO,QAAQ,MAAM,SAAS,CACvD,KAAI,QAAQ,MAAM,kBAAkB,OAAO;EAE9C,WACC,MAAM,SAAS,4CACf,MAAM,SAAS,yCAEf,iBAAiB,KAAK;GACpB,MAAM;GACN,MAAM,MAAM;GACZ,OAAO,MAAM;EACd,EAAC;WAEF,MAAM,SAAS,wCACf,MAAM,SAAS,uCAEf,iBAAiB,EACf,cAAc;GACZ,IAAI,MAAM;GACV,MAAM,MAAM,KAAK,QAAQ,aAAa,GAAG,CAAC,QAAQ,cAAc,GAAG;GACnE,QAAQ;EACT,EACF;WACQ,MAAM,SAAS,yBACxB,kBAAkB,UAAU,MAAM;WAElC,MAAM,SAAS,gCACf,UAAU,SACV,MAAM,KAAK,SAAS,aACpB;GACA,MAAMQ,UAA6D,MAChE,KAAK,UACJ,MAAM,KAAK,QAAQ,IAAI,CAAC,GAAG,WAAW;IACpC,GAAG;IACH;GACD,GAAE,GACH;GAEJ,kBAAkB,YAAY;IAI5B,IAAI,MAAM,KAAK;IACf,MAAM,MAAM,KAAK;IACjB,GAAI,UAAU,EAAE,QAAS,IAAG,CAAE;GAC/B;EACF,WAAU,MAAM,SAAS,yCACxB,kBAAkB,YAAY;GAC5B,MAAM;GACN,SAAS,CAAC;IAAE,GAAG,MAAM;IAAM,OAAO,MAAM;GAAe,CAAC;EACzD;WACQ,MAAM,SAAS,yCACxB,kBAAkB,YAAY;GAC5B,MAAM;GACN,SAAS,CACP;IACE,MAAM,MAAM;IACZ,MAAM;IACN,OAAO,MAAM;GACd,CACF;EACF;WACQ,MAAM,SAAS,+CAGxB,QAAO;MAEP,QAAO;AAGT,SAAO,IAAIC,6CAAoB;GAE7B,MAAM,QAAQ,IAAI,CAAC,SAAS,KAAK,KAAK,CAAC,KAAK,GAAG;GAC/C,SAAS,IAAIC,yCAAe;IAC1B;IACS;IACT;IACA;IACA;IACA;GACD;GACD;EACD;CACF;;CAGD,AAAU,kCAAkC5B,UAAyB;AACnE,SAAO,SAAS,QACd,CAAC,UAAqD;GACpD,MAAM,mBAAmB,MAAM;AAG/B,OAAI,kBAAkB,mBAAmB,KACvC,QAAO6B,kDAAiC,MAAM;GAGhD,MAAM,oBAAoB,MAAM;GAQhC,IAAI,OAAOC,iCAAoB,MAAM;AACrC,OAAI,SAAS,YAAYC,8BAAiB,KAAK,MAAM,EACnD,OAAO;AAET,OAAI,SAAS,WACX,OAAM,IAAI,MACR;AAIJ,OAAI,SAAS,QAAQ;IACnB,MAAM,cAAc;AAGpB,QAAI,mBAAmB,SAAS,wBAAwB;KACtD,MAAM,UAAU,MAAM;AACpB,UAAI,OAAO,YAAY,YAAY,SACjC,QAAO;OACL,MAAM;OACN,WAAW,YAAY;MACxB;AAGH,UAAI,MAAM,QAAQ,YAAY,QAAQ,EAAE;OACtC,MAAM,gBAAgB,YAAY,QAAQ,KACxC,CAAC,MAAM,EAAE,SAAS,sBACnB;AAED,WAAI,cAAe,QAAO;OAE1B,MAAM,UAAU,YAAY,QAAQ,KAClC,CAAC,MAAM,EAAE,SAAS,YACnB;AAED,WAAI,QACF,QAAO;QACL,MAAM;QACN,WACE,OAAO,QAAQ,cAAc,WACzB,QAAQ,YACR,QAAQ,UAAU;OACzB;MAEJ;AAED,YAAM,IAAI,MAAM;KACjB,IAAG;AAEJ,YAAO;MACL,MAAM;MACN;MACA,SAAS,YAAY;KACtB;IACF;AAGD,QAAI,YAAY,mBAAmB,WACjC,QAAO;KACL,MAAM;KACN,SAAS,YAAY;KACrB,QAAQ,YAAY;IACrB;AAGH,WAAO;KACL,MAAM;KACN,SAAS,YAAY;KACrB,IAAI,YAAY,IAAI,WAAW,MAAM,GAAG,YAAY,KAAK;KACzD,QACE,OAAO,YAAY,YAAY,WAC3B,KAAK,UAAU,YAAY,QAAQ,GACnC,YAAY;IACnB;GACF;AAED,OAAI,SAAS,aAAa;AAExB,QACE,CAAC,KAAK,cACN,kBAAkB,UAAU,QAC5B,MAAM,QAAQ,kBAAkB,OAAO,IACvC,kBAAkB,OAAO,SAAS,KAClC,kBAAkB,OAAO,MAAM,CAAC,SAAS,UAAU,KAAK,CAExD,QAAO,kBAAkB;IAK3B,MAAMC,QAA8B,CAAE;AAGtC,QAAI,mBAAmB,aAAa,CAAC,KAAK,YAAY;KACpD,MAAM,gBAAgB,KAAK,yBACzB,kBAAkB,UACnB;KACD,MAAM,KAAK,cAAc;IAC1B;IAGD,IAAI,EAAE,SAAS,GAAG;AAClB,QAAI,mBAAmB,SAAS;AAC9B,SAAI,OAAO,YAAY,UACrB,UAAU,CACR;MAAE,MAAM;MAAe,MAAM;MAAS,aAAa,CAAE;KAAE,CACxD;KAEH,UAAU,CACR,GAAG,SACH;MAAE,MAAM;MAAW,SAAS,kBAAkB;KAAS,CACxD;IACF;AAED,QAAI,OAAO,YAAY,YAAY,QAAQ,SAAS,GAClD,MAAM,KAAK;KACT,MAAM;KACN,MAAM;KACN,GAAI,MAAM,MAAM,CAAC,KAAK,cAAc,MAAM,GAAG,WAAW,OAAO,GAC3D,EAAE,IAAI,MAAM,GAAI,IAChB,CAAE;KACN,SAASC,kBAAK,MAAM;AAClB,UAAI,OAAO,YAAY,SACrB,QAAO;AAET,aAAO,QAAQ,QAAQ,CAAC,SAAS;AAC/B,WAAI,KAAK,SAAS,OAChB,QAAO;QACL,MAAM;QACN,MAAM,KAAK;QACX,aAAa,KAAK,eAAe,CAAE;OACpC;AAGH,WAAI,KAAK,SAAS,iBAAiB,KAAK,SAAS,UAC/C,QAAO;AAGT,cAAO,CAAE;MACV,EAAC;KACH,EAAC;IACH,EAAC;IAGJ,MAAM,kBACJ,oBAAoB;AAEtB,mDAAgB,MAAM,IAAI,CAAC,CAAC,MAAM,YAAY,QAC5C,MAAM,KACJ,GAAG,MAAM,WAAW,IAAI,CAAC,aAAiC;AACxD,SAAIC,+BAAiB,SAAS,CAC5B,QAAO;MACL,MAAM;MACN,IAAI,SAAS;MACb,SAAS,SAAS,MAAM;MACxB,OAAO,SAAS,KAAK;MACrB,MAAM,SAAS;KAChB;AAEH,YAAO;MACL,MAAM;MACN,MAAM,SAAS;MACf,WAAW,KAAK,UAAU,SAAS,KAAK;MACxC,SAAS,SAAS;MAClB,GAAI,KAAK,aACL,EAAE,IAAI,kBAAkB,SAAS,IAAM,IACvC,CAAE;KACP;IACF,EAAC,CACH;aACQ,mBAAmB,YAC5B,MAAM,KACJ,GAAG,kBAAkB,WAAW,IAC9B,CAAC,cAAkC;KACjC,MAAM;KACN,MAAM,SAAS,SAAS;KACxB,SAAS,SAAS;KAClB,WAAW,SAAS,SAAS;KAC7B,GAAI,KAAK,aACL,EAAE,IAAI,kBAAkB,SAAS,IAAK,IACtC,CAAE;IACP,GACF,CACF;IAGH,MAAM,eACJ,kBAAkB,SACjB,SACC,kBAAkB,SAClB,kBAAkB;IAEtB,MAAMC,uBAAqD;KACzD;KACA;KACA;KACA;IACD;AAED,QAAI,eAAe,MAAM;KACvB,MAAM,kBAAkB;KACxB,MAAM,mBAAmB,iBAAiB,OAAO,CAAC,SAChD,qBAAqB,SAAS,KAAK,KAAK,CACzC;AACD,SAAI,iBAAiB,SAAS,GAAG,MAAM,KAAK,GAAG,iBAAiB;IACjE;AAED,WAAO;GACR;AAED,OAAI,SAAS,UAAU,SAAS,YAAY,SAAS,aAAa;AAChE,QAAI,OAAO,MAAM,YAAY,SAC3B,QAAO;KAAE,MAAM;KAAW;KAAM,SAAS,MAAM;IAAS;IAG1D,MAAMC,aAAiC,CAAE;IACzC,MAAM,UAAU,MAAM,QAAQ,QAAQ,CAAC,SAAS;AAC9C,SAAI,KAAK,SAAS,yBAChBC,WAAS,KAAK;MACZ,MAAM;MACN,qBAAqB,KAAK;MAC1B,SAAS,KAAK;KACf,EAAC;AAEJ,2DAAuB,KAAK,CAC1B,qEACE,MACAC,2DACD;AAEH,SAAI,KAAK,SAAS,OAChB,QAAO;MACL,MAAM;MACN,MAAM,KAAK;KACZ;AAEH,SAAI,KAAK,SAAS,aAAa;MAC7B,MAAM,WAAWL,kBAAK,MAAM;AAC1B,WAAI,OAAO,KAAK,cAAc,SAC5B,QAAO,KAAK;gBAEZ,OAAO,KAAK,cAAc,YAC1B,KAAK,cAAc,QACnB,SAAS,KAAK,UAEd,QAAO,KAAK,UAAU;AAExB,cAAO;MACR,EAAC;MACF,MAAM,SAASA,kBAAK,MAAM;AACxB,WAAI,OAAO,KAAK,cAAc,SAC5B,QAAO;gBAEP,OAAO,KAAK,cAAc,YAC1B,KAAK,cAAc,QACnB,YAAY,KAAK,UAEjB,QAAO,KAAK,UAAU;AAExB,cAAO;MACR,EAAC;AACF,aAAO;OACL,MAAM;OACN,WAAW;OACX;MACD;KACF;AACD,SACE,KAAK,SAAS,gBACd,KAAK,SAAS,iBACd,KAAK,SAAS,aAEd,QAAO;AAET,YAAO,CAAE;IACV,EAAC;AAEF,QAAI,QAAQ,SAAS,GACnBI,WAAS,KAAK;KACZ,MAAM;KACN;KACS;IACV,EAAC;AAEJ,WAAOA;GACR;GAED,QAAQ,KACN,CAAC,gEAAgE,EAAE,MAAM,CAC1E;AACD,UAAO,CAAE;EACV,EACF;CACF;;CAGD,AAAU,yBACRE,WAC8C;EAE9C,MAAM,WACJ,UAAU,QAAQ,SAAS,IACvB,UAAU,QAAQ,OAChB,CAAC,KAAK,SAAS;GACb,MAAM,OAAO,IAAI,IAAI,SAAS;AAE9B,OAAI,KAAM,UAAU,KAAK,OACvB,KAAM,QAAQ,KAAK;QAEnB,IAAI,KAAK,KAAK;AAEhB,UAAO;EACR,GACD,CAAC,EAAE,GAAG,UAAU,QAAQ,GAAI,CAAC,EAC9B,GACD,UAAU,SACd,IAAI,CAAC,MACL,OAAO,YAAY,OAAO,QAAQ,EAAE,CAAC,OAAO,CAAC,CAAC,EAAE,KAAK,MAAM,QAAQ,CAAC,CACrE;AAED,SAAO;GACL,GAAG;GACH;EACD;CACF;;CAGD,AAAU,uBACRC,OACAC,QACiB;EACjB,MAAMC,eAAgC,CAAE;AACxC,OAAK,MAAM,QAAQ,MACjB,KAAIC,4BAAc,KAAK,EAAE;AACvB,OAAI,KAAK,SAAS,sBAAsB,QAAQ,QAG9C,KAAK,iBAAiB;GAExB,aAAa,KAAK,KAAK;EACxB,WAAUC,2BAAa,KAAK,EAAE;GAC7B,MAAM,iBAAiB,KAAK,SAAS;GACrC,aAAa,KAAK;IAChB,MAAM;IACN,MAAM,eAAe;IACrB,aAAa,eAAe;IAC5B,QAAQ,eAAe;GACxB,EAAkB;EACpB,mEAA+B,KAAK,EACnC,aAAa,KAAK;GAChB,MAAM;GACN,MAAM,KAAK,SAAS;GACpB,YAAY,KAAK,SAAS;GAC1B,aAAa,KAAK,SAAS;GAC3B,QAAQ,QAAQ,UAAU;EAC3B,EAAC;WACOC,iCAAmB,KAAK,EACjC,aAAa,KAAKC,2CAA6B,KAAK,CAAC;AAGzD,SAAO;CACR;AACF"}