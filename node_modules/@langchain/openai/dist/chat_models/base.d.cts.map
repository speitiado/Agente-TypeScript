{"version":3,"file":"base.d.cts","names":["____________langchain_core_dist_utils_stream_js0","ClientOptions","OpenAI","OpenAIClient","AIMessageChunk","BaseMessage","ChatGeneration","BaseChatModel","LangSmithParams","BaseChatModelParams","BaseChatModelCallOptions","BaseFunctionCallOptions","BaseLanguageModelInput","StructuredOutputMethodOptions","Runnable","InteropZodType","OpenAICallOptions","OpenAIChatInput","OpenAICoreRequestOptions","ChatOpenAIResponseFormat","ResponseFormatConfiguration","OpenAIVerbosityParam","OpenAIToolChoice","ChatOpenAIToolType","ResponsesToolChoice","OpenAILLMOutput","BaseChatOpenAICallOptions","Chat","ChatCompletionStreamOptions","ChatCompletionModality","Array","ChatCompletionAudioParam","ChatCompletionPredictionContent","Reasoning","ChatCompletionCreateParams","BaseChatOpenAIFields","Partial","BaseChatOpenAI","CallOptions","Record","Omit","ChatCompletionTool","_langchain_core_messages0","MessageStructure","IterableReadableStream","Promise","Function","ChatCompletionFunctionCallOption","RunOutput"],"sources":["../../src/chat_models/base.d.ts"],"sourcesContent":["import { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { type ChatGeneration } from \"@langchain/core/outputs\";\nimport { BaseChatModel, type LangSmithParams, type BaseChatModelParams, BaseChatModelCallOptions } from \"@langchain/core/language_models/chat_models\";\nimport { type BaseFunctionCallOptions, type BaseLanguageModelInput, type StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { InteropZodType } from \"@langchain/core/utils/types\";\nimport { type OpenAICallOptions, type OpenAIChatInput, type OpenAICoreRequestOptions, type ChatOpenAIResponseFormat, ResponseFormatConfiguration, OpenAIVerbosityParam } from \"../types.js\";\nimport { OpenAIToolChoice, ChatOpenAIToolType, ResponsesToolChoice } from \"../utils/tools.js\";\ninterface OpenAILLMOutput {\n    tokenUsage: {\n        completionTokens?: number;\n        promptTokens?: number;\n        totalTokens?: number;\n    };\n}\nexport type { OpenAICallOptions, OpenAIChatInput };\nexport interface BaseChatOpenAICallOptions extends BaseChatModelCallOptions, BaseFunctionCallOptions {\n    /**\n     * Additional options to pass to the underlying axios request.\n     */\n    options?: OpenAICoreRequestOptions;\n    /**\n     * A list of tools that the model may use to generate responses.\n     * Each tool can be a function, a built-in tool, or a custom tool definition.\n     * If not provided, the model will not use any tools.\n     */\n    tools?: ChatOpenAIToolType[];\n    /**\n     * Specifies which tool the model should use to respond.\n     * Can be an {@link OpenAIToolChoice} or a {@link ResponsesToolChoice}.\n     * If not set, the model will decide which tool to use automatically.\n     */\n    // TODO: break OpenAIToolChoice and ResponsesToolChoice into options sub classes\n    tool_choice?: OpenAIToolChoice | ResponsesToolChoice;\n    /**\n     * Adds a prompt index to prompts passed to the model to track\n     * what prompt is being used for a given generation.\n     */\n    promptIndex?: number;\n    /**\n     * An object specifying the format that the model must output.\n     */\n    response_format?: ChatOpenAIResponseFormat;\n    /**\n     * When provided, the completions API will make a best effort to sample\n     * deterministically, such that repeated requests with the same `seed`\n     * and parameters should return the same result.\n     */\n    seed?: number;\n    /**\n     * Additional options to pass to streamed completions.\n     * If provided, this takes precedence over \"streamUsage\" set at\n     * initialization time.\n     */\n    stream_options?: OpenAIClient.Chat.ChatCompletionStreamOptions;\n    /**\n     * The model may choose to call multiple functions in a single turn. You can\n     * set parallel_tool_calls to false which ensures only one tool is called at most.\n     * [Learn more](https://platform.openai.com/docs/guides/function-calling#parallel-function-calling)\n     */\n    parallel_tool_calls?: boolean;\n    /**\n     * If `true`, model output is guaranteed to exactly match the JSON Schema\n     * provided in the tool definition. If `true`, the input schema will also be\n     * validated according to\n     * https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n     *\n     * If `false`, input schema will not be validated and model output will not\n     * be validated.\n     *\n     * If `undefined`, `strict` argument will not be passed to the model.\n     */\n    strict?: boolean;\n    /**\n     * Output types that you would like the model to generate for this request. Most\n     * models are capable of generating text, which is the default:\n     *\n     * `[\"text\"]`\n     *\n     * The `gpt-4o-audio-preview` model can also be used to\n     * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n     * this model generate both text and audio responses, you can use:\n     *\n     * `[\"text\", \"audio\"]`\n     */\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    /**\n     * Parameters for audio output. Required when audio output is requested with\n     * `modalities: [\"audio\"]`.\n     * [Learn more](https://platform.openai.com/docs/guides/audio).\n     */\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    /**\n     * Static predicted output content, such as the content of a text file that is being regenerated.\n     * [Learn more](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs).\n     */\n    prediction?: OpenAIClient.ChatCompletionPredictionContent;\n    /**\n     * Options for reasoning models.\n     *\n     * Note that some options, like reasoning summaries, are only available when using the responses\n     * API. If these options are set, the responses API will be used to fulfill the request.\n     *\n     * These options will be ignored when not using a reasoning model.\n     */\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\"\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates. Replaces the `user` field.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey?: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n}\nexport interface BaseChatOpenAIFields extends Partial<OpenAIChatInput>, BaseChatModelParams {\n    /**\n     * Optional configuration options for the OpenAI client.\n     */\n    configuration?: ClientOptions;\n}\n/** @internal */\nexport declare abstract class BaseChatOpenAI<CallOptions extends BaseChatOpenAICallOptions> extends BaseChatModel<CallOptions, AIMessageChunk> implements Partial<OpenAIChatInput> {\n    temperature?: number;\n    topP?: number;\n    frequencyPenalty?: number;\n    presencePenalty?: number;\n    n?: number;\n    logitBias?: Record<string, number>;\n    model: string;\n    modelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n    stop?: string[];\n    stopSequences?: string[];\n    user?: string;\n    timeout?: number;\n    streaming: boolean;\n    streamUsage: boolean;\n    maxTokens?: number;\n    logprobs?: boolean;\n    topLogprobs?: number;\n    apiKey?: string;\n    organization?: string;\n    __includeRawResponse?: boolean;\n    /** @internal */\n    client: OpenAIClient;\n    /** @internal */\n    clientConfig: ClientOptions;\n    /**\n     * Whether the model supports the `strict` argument when passing in tools.\n     * If `undefined` the `strict` argument will not be passed to OpenAI.\n     */\n    supportsStrictToolCalling?: boolean;\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable\n     * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your\n     * OpenAI organization or project. This must be configured directly with OpenAI.\n     *\n     * See:\n     * https://platform.openai.com/docs/guides/your-data\n     * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n     *\n     * @default false\n     */\n    zdrEnabled?: boolean | undefined;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n    protected defaultOptions: CallOptions;\n    _llmType(): string;\n    static lc_name(): string;\n    get callKeys(): string[];\n    lc_serializable: boolean;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_aliases(): Record<string, string>;\n    get lc_serializable_keys(): string[];\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    /** @ignore */\n    _identifyingParams(): Omit<OpenAIClient.Chat.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams(): Omit<OpenAIClient.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    constructor(fields?: BaseChatOpenAIFields);\n    /**\n     * Returns backwards compatible reasoning parameters from constructor params and call options\n     * @internal\n     */\n    protected _getReasoningParams(options?: this[\"ParsedCallOptions\"]): OpenAIClient.Reasoning | undefined;\n    /**\n     * Returns an openai compatible response format from a set of options\n     * @internal\n     */\n    protected _getResponseFormat(resFormat?: CallOptions[\"response_format\"]): ResponseFormatConfiguration | undefined;\n    protected _combineCallOptions(additionalOptions?: this[\"ParsedCallOptions\"]): this[\"ParsedCallOptions\"];\n    /** @internal */\n    _getClientOptions(options: OpenAICoreRequestOptions | undefined): OpenAICoreRequestOptions;\n    // TODO: move to completions class\n    protected _convertChatOpenAIToolToCompletionsTool(tool: ChatOpenAIToolType, fields?: {\n        strict?: boolean;\n    }): OpenAIClient.ChatCompletionTool;\n    bindTools(tools: ChatOpenAIToolType[], kwargs?: Partial<CallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions>;\n    stream(input: BaseLanguageModelInput, options?: CallOptions): Promise<import(\"../../../../langchain-core/dist/utils/stream.js\").IterableReadableStream<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>>;\n    invoke(input: BaseLanguageModelInput, options?: CallOptions): Promise<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>;\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput;\n    getNumTokensFromMessages(messages: BaseMessage[]): Promise<{\n        totalCount: number;\n        countPerMessage: number[];\n    }>;\n    /** @internal */\n    protected _getNumTokensFromGenerations(generations: ChatGeneration[]): Promise<number>;\n    /** @internal */\n    protected _getEstimatedTokenCountFromPrompt(messages: BaseMessage[], functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[], function_call?: \"none\" | \"auto\" | OpenAIClient.Chat.ChatCompletionFunctionCallOption): Promise<number>;\n    /** @internal */\n    protected _getStructuredOutputMethod(config: StructuredOutputMethodOptions<boolean>): string | undefined;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<boolean>): Runnable<BaseLanguageModelInput, RunOutput> | Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n}\n"],"mappings":";;;;;;;;;;;;;UASUyB,eAAAA;;;;IAAAA,WAAAA,CAAAA,EAAe,MAAA;EAQRC,CAAAA;;AAULH,UAVKG,yBAAAA,SAAkChB,wBAUvCa,EAViEZ,uBAUjEY,CAAAA;EAAkB;;;EAgBgB,OAYzBpB,CAAAA,EAlCPe,wBAkCyBU;EAA2B;;;;;EAmD5B,KAKnBzB,CAAAA,EApFPoB,kBAoFyBW,EAAAA;EAA0B;;;AA9FqC;AA0GpG;EAAqC;EAAA,WAAiBjB,CAAAA,EAzFpCK,gBAyFoCL,GAzFjBO,mBAyFiBP;EAAe;;;AAAsB;EAO7DoB,WAAAA,CAAAA,EAAAA,MAAc;EAAA;;;EAAiF,eAAEjC,CAAAA,EAvFzGe,wBAuFyGf;EAAc;;;;;EAwB9G,IAMnBD,CAAAA,EAAAA,MAAawB;EAA6B;;;;;EA6BlB,cACNW,CAAAA,EAvITnC,QAAAA,CAAawB,IAAAA,CAAKC,2BAuITU;EAAW;;;;;EAcpB,mBAIsBJ,CAAAA,EAAAA,OAAAA;EAA0B;;;;;;;;;;;EAqB9B,MAAqBI,CAAAA,EAAAA,OAAAA;EAAW;;;;;;;;;;;;EAE/B,UAAYA,CAAAA,EAjJnCR,KAiJmCQ,CAjJ7BnC,QAAAA,CAAawB,IAAAA,CAAKE,sBAiJWS,CAAAA;EAAW;;;;;EAES,KACjCjC,CAAAA,EA9I3BF,QAAAA,CAAawB,IAAAA,CAAKI,wBA8IS1B;EAAW;;;;EAOmB,UAAgBF,CAAAA,EAhJpEA,QAAAA,CAAa6B,+BAgJoGc;EAAQ;;;;;;;;EAO7H,SAAwBjC,CAAAA,EA9IrBV,QAAAA,CAAa8B,SA8IQpB;EAA6B;;;;EAGtC,YAAgB0B,CAAAA,EA5IzBpC,QAAAA,CAAawB,IAAAA,CAAKO,0BA4IOK,CAAAA,cAAAA,CAAAA;EAAM;;;;;EAEwD,cAC7FlC,CAAAA,EAAAA,MAAAA;EAAW;;;EAKI,SAAgBkC,CAAAA,EA1I5BlB,oBA0I4BkB;;AAAmCxB,UAxI9DoB,oBAAAA,SAA6BC,OAwIiCrB,CAxIzBE,eAwIyBF,CAAAA,EAxIPN,mBAwIOM,CAAAA;EAAc;;;EAEgB,aAAEiC,CAAAA,EAtI3F/C,aAsI2F+C;;;AAClG3C,uBApIiBgC,cAoIjBhC,CAAAA,oBApIoDqB,yBAoIpDrB,CAAAA,SApIuFE,aAoIvFF,CApIqGiC,WAoIrGjC,EApIkHD,cAoIlHC,CAAAA,YApI6I+B,OAoI7I/B,CApIqJY,eAoIrJZ,CAAAA,CAAAA;EAAW,WACR2C,CAAAA,EAAAA,MAAAA;EAAS,IAFmGlC,CAAAA,EAAAA,MAAAA;EAAQ,gBAnIhCP,CAAAA,EAAAA,MAAAA;EAAa,eAAyC6B,CAAAA,EAAAA,MAAAA;EAAO,CAAA,CAAA,EAAA,MAAA;cAMjJG;;gBAEEtB;;;;;;;;;;;;;;UAcNd;;gBAEMF;;;;;;UAMNE,QAAAA,CAAawB,IAAAA,CAAKI;eACbD,MAAM3B,QAAAA,CAAawB,IAAAA,CAAKE;cACzB1B,QAAAA,CAAa8B;;;;;;;;;;;;;;;;;iBAiBV9B,QAAAA,CAAawB,IAAAA,CAAKO;;;;;;;;;;cAUrBb;4BACciB;;;;;;;;oBAQRC;;mDAE+B/B;;wBAE3BgC,KAAKrC,QAAAA,CAAawB,IAAAA,CAAKO;;MAEzCjC;;;;uBAIiBuC,KAAKrC,QAAAA,CAAa+B;;MAEnCjC;uBACiBkC;;;;;sEAK+ChC,QAAAA,CAAa8B;;;;;2CAKxCK,iCAAiClB;;;6BAG/CF,uCAAuCA;;0DAEVK;;MAEpDpB,QAAAA,CAAasC;mBACAlB,+BAA+Ba,QAAQE,eAAexB,SAASF,wBAAwBR,gBAAgBkC;gBAC1G1B,kCAAkC0B,cAAcO,QAAuG,uBAAdzC,eAA5FsC,yBAAAA,CAA8IC,gBAAAA;gBAC3L/B,kCAAkC0B,cAAcO,QAAQzC,eAAXsC,yBAAAA,CAA6DC,gBAAAA;;mCAEvFlB,oBAAoBA;qCAClBpB,gBAAgBwC;;;;;sDAKCvC,mBAAmBuC;;wDAEjBxC,2BAA2BF,QAAAA,CAAawB,IAAAA,CAAKO,0BAAAA,CAA2BY,8CAA8C3C,QAAAA,CAAawB,IAAAA,CAAKoB,mCAAmCF;;+CAEpLhC;;;oBAG3B0B,sBAAsBA,mCAAmCxB,eAAeiC;;IAEvFT,8BAA8B1B,uCAAuCC,SAASF,wBAAwBoC;;;oBAGvFT,sBAAsBA,mCAAmCxB,eAAeiC;;IAEvFT,8BAA8B1B,sCAAsCC,SAASF;SACvEP;YACG2C;;;;oBAIMT,sBAAsBA,mCAAmCxB,eAAeiC;;IAEvFT,8BAA8B1B,yCAAyCC,SAASF,wBAAwBoC,aAAalC,SAASF;SACxHP;YACG2C"}