import { ChatOpenAIReasoningSummary, OpenAIVerbosityParam } from "../types.js";
import { ChatOpenAIToolType, ResponsesTool } from "../utils/tools.js";
import { BaseChatOpenAI, BaseChatOpenAICallOptions } from "./base.js";
import { OpenAI as OpenAI$1 } from "openai";
import { BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk, ChatResult } from "@langchain/core/outputs";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";

//#region src/chat_models/responses.d.ts
interface ChatOpenAIResponsesCallOptions extends BaseChatOpenAICallOptions {
  /**
   * Configuration options for a text response from the model. Can be plain text or
   * structured JSON data.
   */
  text?: OpenAI$1.Responses.ResponseCreateParams["text"];
  /**
   * The truncation strategy to use for the model response.
   */
  truncation?: OpenAI$1.Responses.ResponseCreateParams["truncation"];
  /**
   * Specify additional output data to include in the model response.
   */
  include?: OpenAI$1.Responses.ResponseCreateParams["include"];
  /**
   * The unique ID of the previous response to the model. Use this to create multi-turn
   * conversations.
   */
  previous_response_id?: OpenAI$1.Responses.ResponseCreateParams["previous_response_id"];
  /**
   * The verbosity of the model's response.
   */
  verbosity?: OpenAIVerbosityParam;
}
type ChatResponsesInvocationParams = Omit<OpenAI$1.Responses.ResponseCreateParams, "input">;
type ExcludeController<T> = T extends {
  controller: unknown;
} ? never : T;
type ResponsesCreate = OpenAI$1.Responses["create"];
type ResponsesParse = OpenAI$1.Responses["parse"];
type ResponsesCreateInvoke = ExcludeController<Awaited<ReturnType<ResponsesCreate>>>;
type ResponsesParseInvoke = ExcludeController<Awaited<ReturnType<ResponsesParse>>>;
/**
 * OpenAI Responses API implementation.
 *
 * Will be exported in a later version of @langchain/openai.
 *
 * @internal
 */
declare class ChatOpenAIResponses<CallOptions extends ChatOpenAIResponsesCallOptions = ChatOpenAIResponsesCallOptions> extends BaseChatOpenAI<CallOptions> {
  invocationParams(options?: this["ParsedCallOptions"]): ChatResponsesInvocationParams;
  _generate(messages: BaseMessage[], options: this["ParsedCallOptions"]): Promise<ChatResult>;
  _streamResponseChunks(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
  /**
   * Calls the Responses API with retry logic in case of failures.
   * @param request The request to send to the OpenAI API.
   * @param options Optional configuration for the API call.
   * @returns The response from the OpenAI API.
   */
  completionWithRetry(request: OpenAI$1.Responses.ResponseCreateParamsStreaming, requestOptions?: OpenAI$1.RequestOptions): Promise<AsyncIterable<OpenAI$1.Responses.ResponseStreamEvent>>;
  completionWithRetry(request: OpenAI$1.Responses.ResponseCreateParamsNonStreaming, requestOptions?: OpenAI$1.RequestOptions): Promise<OpenAI$1.Responses.Response>;
  /** @internal */
  protected _convertResponsesMessageToBaseMessage(response: ResponsesCreateInvoke | ResponsesParseInvoke): BaseMessage;
  /** @internal */
  protected _convertResponsesDeltaToBaseMessageChunk(chunk: OpenAI$1.Responses.ResponseStreamEvent): ChatGenerationChunk | null;
  /** @internal */
  protected _convertMessagesToResponsesParams(messages: BaseMessage[]): (OpenAI$1.Responses.ResponseInputItem.ComputerCallOutput | OpenAI$1.Responses.EasyInputMessage | OpenAI$1.Responses.ResponseInputItem.FunctionCallOutput | OpenAI$1.Responses.ResponseInputItem.ImageGenerationCall | OpenAI$1.Responses.ResponseInputItem.ItemReference | OpenAI$1.Responses.ResponseInputItem.LocalShellCall | OpenAI$1.Responses.ResponseInputItem.LocalShellCallOutput | OpenAI$1.Responses.ResponseInputItem.McpApprovalRequest | OpenAI$1.Responses.ResponseInputItem.McpApprovalResponse | OpenAI$1.Responses.ResponseInputItem.McpCall | OpenAI$1.Responses.ResponseInputItem.McpListTools | OpenAI$1.Responses.ResponseInputItem.Message | OpenAI$1.Responses.ResponseCodeInterpreterToolCall | OpenAI$1.Responses.ResponseComputerToolCall | OpenAI$1.Responses.ResponseCustomToolCall | OpenAI$1.Responses.ResponseCustomToolCallOutput | OpenAI$1.Responses.ResponseFileSearchToolCall | OpenAI$1.Responses.ResponseFunctionToolCall | OpenAI$1.Responses.ResponseFunctionWebSearch | OpenAI$1.Responses.ResponseOutputMessage | OpenAI$1.Responses.ResponseReasoningItem)[];
  /** @internal */
  protected _convertReasoningSummary(reasoning: ChatOpenAIReasoningSummary): OpenAI$1.Responses.ResponseReasoningItem;
  /** @internal */
  protected _reduceChatOpenAITools(tools: ChatOpenAIToolType[], fields: {
    stream?: boolean;
    strict?: boolean;
  }): ResponsesTool[];
}
//#endregion
export { ChatOpenAIResponses, ChatOpenAIResponsesCallOptions };
//# sourceMappingURL=responses.d.ts.map