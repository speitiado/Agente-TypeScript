{"version":3,"file":"chat_models.d.cts","names":["BaseMessage","BaseMessageChunk","BaseMessageLike","AIMessageChunk","MessageOutputVersion","ChatGenerationChunk","ChatResult","Generation","LLMResult","BaseCache","CallbackManagerForLLMRun","Callbacks","RunnableConfig","Runnable","RunnableToolLike","BasePromptValueInterface","BaseLanguageModel","BaseLanguageModelCallOptions","BaseLanguageModelInput","BaseLanguageModelParams","StructuredOutputMethodOptions","ToolDefinition","StructuredToolInterface","StructuredToolParams","ZodType","$ZodType","ToolChoice","Record","SerializedChatModel","SerializedLLM","BaseChatModelParams","BaseChatModelCallOptions","LangSmithParams","Array","BindToolsInput","BaseChatModel","OutputMessageType","CallOptions","Exclude","Omit","Partial","Promise","AsyncGenerator","messages","cache","llmStringKey","parsedOptions","handledOptions","RunOutput","SimpleChatModel"],"sources":["../../../../../../langchain-core/dist/language_models/chat_models.d.ts"],"sourcesContent":["import { BaseMessage, BaseMessageChunk, BaseMessageLike } from \"../messages/base.js\";\nimport { AIMessageChunk } from \"../messages/ai.js\";\nimport { MessageOutputVersion } from \"../messages/message.js\";\nimport { ChatGenerationChunk, ChatResult, Generation, LLMResult } from \"../outputs.js\";\nimport { BaseCache } from \"../caches/base.js\";\nimport { CallbackManagerForLLMRun, Callbacks } from \"../callbacks/manager.js\";\nimport { RunnableConfig } from \"../runnables/types.js\";\nimport { Runnable, RunnableToolLike } from \"../runnables/base.js\";\nimport { BasePromptValueInterface } from \"../prompt_values.js\";\nimport { BaseLanguageModel, BaseLanguageModelCallOptions, BaseLanguageModelInput, BaseLanguageModelParams, StructuredOutputMethodOptions, ToolDefinition } from \"./base.js\";\nimport { StructuredToolInterface, StructuredToolParams } from \"../tools/types.js\";\nimport { ZodType } from \"zod/v3\";\nimport { $ZodType } from \"zod/v4/core\";\n\n//#region src/language_models/chat_models.d.ts\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\ntype ToolChoice = string | Record<string, any> | \"auto\" | \"any\";\n/**\n * Represents a serialized chat model.\n */\ntype SerializedChatModel = {\n  _model: string;\n  _type: string;\n} & Record<string, any>;\n// todo?\n/**\n * Represents a serialized large language model.\n */\ntype SerializedLLM = {\n  _model: string;\n  _type: string;\n} & Record<string, any>;\n/**\n * Represents the parameters for a base chat model.\n */\ntype BaseChatModelParams = BaseLanguageModelParams & {\n  /**\n   * Whether to disable streaming.\n   *\n   * If streaming is bypassed, then `stream()` will defer to\n   * `invoke()`.\n   *\n   * - If true, will always bypass streaming case.\n   * - If false (default), will always use streaming case if available.\n   */\n  disableStreaming?: boolean;\n  /**\n   * Version of `AIMessage` output format to store in message content.\n   *\n   * `AIMessage.contentBlocks` will lazily parse the contents of `content` into a\n   * standard format. This flag can be used to additionally store the standard format\n   * as the message content, e.g., for serialization purposes.\n   *\n   * - \"v0\": provider-specific format in content (can lazily parse with `.contentBlocks`)\n   * - \"v1\": standardized format in content (consistent with `.contentBlocks`)\n   *\n   * You can also set `LC_OUTPUT_VERSION` as an environment variable to \"v1\" to\n   * enable this by default.\n   *\n   * @default \"v0\"\n   */\n  outputVersion?: MessageOutputVersion;\n};\n/**\n * Represents the call options for a base chat model.\n */\ntype BaseChatModelCallOptions = BaseLanguageModelCallOptions & {\n  /**\n   * Specifies how the chat model should use tools.\n   * @default undefined\n   *\n   * Possible values:\n   * - \"auto\": The model may choose to use any of the provided tools, or none.\n   * - \"any\": The model must use one of the provided tools.\n   * - \"none\": The model must not use any tools.\n   * - A string (not \"auto\", \"any\", or \"none\"): The name of a specific tool the model must use.\n   * - An object: A custom schema specifying tool choice parameters. Specific to the provider.\n   *\n   * Note: Not all providers support tool_choice. An error will be thrown\n   * if used with an unsupported model.\n   */\n  tool_choice?: ToolChoice;\n  /**\n   * Version of `AIMessage` output format to store in message content.\n   *\n   * `AIMessage.contentBlocks` will lazily parse the contents of `content` into a\n   * standard format. This flag can be used to additionally store the standard format\n   * as the message content, e.g., for serialization purposes.\n   *\n   * - \"v0\": provider-specific format in content (can lazily parse with `.contentBlocks`)\n   * - \"v1\": standardized format in content (consistent with `.contentBlocks`)\n   *\n   * You can also set `LC_OUTPUT_VERSION` as an environment variable to \"v1\" to\n   * enable this by default.\n   *\n   * @default \"v0\"\n   */\n  outputVersion?: MessageOutputVersion;\n};\ntype LangSmithParams = {\n  ls_provider?: string;\n  ls_model_name?: string;\n  ls_model_type: \"chat\";\n  ls_temperature?: number;\n  ls_max_tokens?: number;\n  ls_stop?: Array<string>;\n};\ntype BindToolsInput = StructuredToolInterface\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n| Record<string, any> | ToolDefinition | RunnableToolLike | StructuredToolParams;\n/**\n * Base class for chat models. It extends the BaseLanguageModel class and\n * provides methods for generating chat based on input messages.\n */\ndeclare abstract class BaseChatModel<CallOptions extends BaseChatModelCallOptions = BaseChatModelCallOptions,\n// TODO: Fix the parameter order on the next minor version.\nOutputMessageType extends BaseMessageChunk = AIMessageChunk> extends BaseLanguageModel<OutputMessageType, CallOptions> {\n  // Backwards compatibility since fields have been moved to RunnableConfig\n  ParsedCallOptions: Omit<CallOptions, Exclude<keyof RunnableConfig, \"signal\" | \"timeout\" | \"maxConcurrency\">>;\n  // Only ever instantiated in main LangChain\n  lc_namespace: string[];\n  disableStreaming: boolean;\n  outputVersion?: MessageOutputVersion;\n  get callKeys(): string[];\n  constructor(fields: BaseChatModelParams);\n  _combineLLMOutput?(...llmOutputs: LLMResult[\"llmOutput\"][]): LLMResult[\"llmOutput\"];\n  protected _separateRunnableConfigFromCallOptionsCompat(options?: Partial<CallOptions>): [RunnableConfig, this[\"ParsedCallOptions\"]];\n  /**\n   * Bind tool-like objects to this chat model.\n   *\n   * @param tools A list of tool definitions to bind to this chat model.\n   * Can be a structured tool, an OpenAI formatted tool, or an object\n   * matching the provider's specific tool schema.\n   * @param kwargs Any additional parameters to bind.\n   */\n  bindTools?(tools: BindToolsInput[], kwargs?: Partial<CallOptions>): Runnable<BaseLanguageModelInput, OutputMessageType, CallOptions>;\n  /**\n   * Invokes the chat model with a single input.\n   * @param input The input for the language model.\n   * @param options The call options.\n   * @returns A Promise that resolves to a BaseMessageChunk.\n   */\n  invoke(input: BaseLanguageModelInput, options?: CallOptions): Promise<OutputMessageType>;\n  // eslint-disable-next-line require-yield\n  _streamResponseChunks(_messages: BaseMessage[], _options: this[\"ParsedCallOptions\"], _runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n  _streamIterator(input: BaseLanguageModelInput, options?: CallOptions): AsyncGenerator<OutputMessageType>;\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n  /** @ignore */\n  _generateUncached(messages: BaseMessageLike[][], parsedOptions: this[\"ParsedCallOptions\"], handledOptions: RunnableConfig, startedRunManagers?: CallbackManagerForLLMRun[]): Promise<LLMResult>;\n  _generateCached({\n    messages,\n    cache,\n    llmStringKey,\n    parsedOptions,\n    handledOptions\n  }: {\n    messages: BaseMessageLike[][];\n    cache: BaseCache<Generation[]>;\n    llmStringKey: string;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    parsedOptions: any;\n    handledOptions: RunnableConfig;\n  }): Promise<LLMResult & {\n    missingPromptIndices: number[];\n    startedRunManagers?: CallbackManagerForLLMRun[];\n  }>;\n  /**\n   * Generates chat based on the input messages.\n   * @param messages An array of arrays of BaseMessage instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  generate(messages: BaseMessageLike[][], options?: string[] | CallOptions, callbacks?: Callbacks): Promise<LLMResult>;\n  /**\n   * Get the parameters used to invoke the model\n   */\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  invocationParams(_options?: this[\"ParsedCallOptions\"]): any;\n  _modelType(): string;\n  abstract _llmType(): string;\n  /**\n   * Generates a prompt based on the input prompt values.\n   * @param promptValues An array of BasePromptValue instances.\n   * @param options The call options or an array of stop sequences.\n   * @param callbacks The callbacks for the language model.\n   * @returns A Promise that resolves to an LLMResult.\n   */\n  generatePrompt(promptValues: BasePromptValueInterface[], options?: string[] | CallOptions, callbacks?: Callbacks): Promise<LLMResult>;\n  abstract _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n  withStructuredOutput<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: $ZodType<RunOutput>\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n  withStructuredOutput<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: $ZodType<RunOutput>\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n    raw: BaseMessage;\n    parsed: RunOutput;\n  }>;\n  withStructuredOutput<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: ZodType<RunOutput>\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n  withStructuredOutput<\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: ZodType<RunOutput>\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n    raw: BaseMessage;\n    parsed: RunOutput;\n  }>;\n}\n/**\n * An abstract class that extends BaseChatModel and provides a simple\n * implementation of _generate.\n */\ndeclare abstract class SimpleChatModel<CallOptions extends BaseChatModelCallOptions = BaseChatModelCallOptions> extends BaseChatModel<CallOptions> {\n  abstract _call(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<string>;\n  _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n}\n//#endregion\nexport { BaseChatModel, BaseChatModelCallOptions, BaseChatModelParams, BindToolsInput, LangSmithParams, SerializedChatModel, SerializedLLM, SimpleChatModel, ToolChoice };\n//# sourceMappingURL=chat_models.d.ts.map"],"mappings":";;KAmGKgC,eAAAA;;;;;;YAMOC"}